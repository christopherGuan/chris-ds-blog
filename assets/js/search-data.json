{
  
    
        "post0": {
            "title": "Introduction to stylometry with Python",
            "content": "Data cleaning . papers = { &#39;Madison&#39;: [10, 14, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48], &#39;Hamilton&#39;: [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], &#39;Jay&#39;: [2, 3, 4, 5], &#39;Shared&#39;: [18, 19, 20], &#39;Disputed&#39;: [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63], &#39;TestCase&#39;: [64] } . def read_files_into_string(filenames): strings = [] for filename in filenames: with open(f&#39;data/federalist_{filename}.txt&#39;) as f: strings.append(f.read()) return &#39; n&#39;.join(strings) . federalist_by_author = {} for author, files in papers.items(): federalist_by_author[author] = read_files_into_string(files) . for author in papers: print(federalist_by_author[author][:100]) . 10 The Same Subject Continued (The Union as a Safeguard Against Domestic Faction and Insurrection) 1 General Introduction For the Independent Journal. Saturday, October 27, 1787 HAMILTON To the 2 Concerning Dangers from Foreign Force and Influence For the Independent Journal. Wednesday, Oct 18 The Same Subject Continued (The Insufficiency of the Present Confederation to Preserve the Unio 49 Method of Guarding Against the Encroachments of Any One Department of Government by Appealing t 64 The Powers of the Senate From The Independent Journal. Wednesday, March 5, 1788. JAY To the . Mendenhall&#8217;s Characteristic Curves of Composition . Mendenhall&#39;s methos nowadays looks very limited. He believes that each writer&#39;s writing style could be find by calculating the frequency with which he or she used words of various lengths. In that case, we can find that author &quot;A&quot; particularly like to use words with 5 or anyother number of charactors. That is his or her secrete key. The main issue with this approach is that Mendenhall&#39;s method does not take into account the actual words in an author&#39;s vocabulary. . In the cell below, we first uses nltk&#39;s word tokenize() method to break down an author&#39;s corpus into its component tokens, such as words, numbers, punctuation, and so on. Second, we eliminates non-words and generates a list containing the lengths of each remaining word token. Next, it generates a frequency distribution object and count the number of one-letter words, two-letter words, and so on in the author&#39;s corpus. Eventually, it creates a graph of the distribution of word lengths in the corpus for all words up to 15 characters long. . import nltk %matplotlib inline # Compare the disputed papers to those written by everyone, # including the shared ones. authors = (&quot;Hamilton&quot;, &quot;Madison&quot;, &quot;Disputed&quot;, &quot;Jay&quot;, &quot;Shared&quot;) # Transform the authors&#39; corpora into lists of word tokens federalist_by_author_tokens = {} federalist_by_author_length_distributions = {} for author in authors: tokens = nltk.word_tokenize(federalist_by_author[author]) # Filter out punctuation federalist_by_author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)]) # Get a distribution of token lengths token_lengths = [len(token) for token in federalist_by_author_tokens[author]] federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths) federalist_by_author_length_distributions[author].plot(15,title=author) . Personal question . Why not just calculate how often some very special words an author used. I guess every author will have some signiture words, and when these words appears more frequent, peopel can identify different author&#39;s differnt writing style. For example, &quot;leniency&quot;, &quot;malicious&quot;, and &quot;coherent&quot; are three relatively advanced words I particularly like to use in my literary paper, so, by using Mendenhall&#39;s method, data analist can find that these three words have been used very frequent in an anonymous paper. Thus, people can identify that this is Zeyu&#39;s masterpiece with a very large confident interval range. . Kilgariff&#8217;s Chi-Squared Method . In the following content, we will use the chi-squared test to measure the “distance” between the vocabularies employed in two sets of texts. The more similar the vocabularies, the more likely the texts in both sets were written by the same author. . . χ2 = Chi-Square value | Oi = Observed frequency (author_count) | Ei = Expected frequency (expected_author_coun) | . In this equation, O is the observed value(author_count), E is the expected value(expected_author_count) and “i” is the “ith” position in the contingency table. . A low value for chi-square means there is a high correlation between your two sets of data. In theory, if your observed and expected values were equal (“no difference”) then chi-square would be zero — an event that is unlikely to happen in real life. Deciding whether a chi-square test statistic is large enough to indicate a statistically significant difference isn’t as easy it seems. . A YouTube video to help you understand better. . authors = (&quot;Hamilton&quot;, &quot;Madison&quot;) # Lowercase the tokens so that the same word, capitalized or not, # counts as one word for author in authors: federalist_by_author_tokens[author] = ( [token.lower() for token in federalist_by_author_tokens[author]]) federalist_by_author_tokens[&quot;Disputed&quot;] = ( [token.lower() for token in federalist_by_author_tokens[&quot;Disputed&quot;]]) # Calculate chisquared for each of the two candidate authors for author in authors: # First, build a joint corpus and identify the 500 most frequent words in it joint_corpus = (federalist_by_author_tokens[author] + federalist_by_author_tokens[&quot;Disputed&quot;]) joint_freq_dist = nltk.FreqDist(joint_corpus) most_common = list(joint_freq_dist.most_common(500)) # What proportion of the joint corpus is made up # of the candidate author&#39;s tokens? author_share = (len(federalist_by_author_tokens[author]) / len(joint_corpus)) # Now, let&#39;s look at the 500 most common words in the candidate # author&#39;s corpus and compare the number of times they can be observed # to what would be expected if the author&#39;s papers # and the Disputed papers were both random samples from the same distribution. chisquared = 0 for word,joint_count in most_common: # How often do we really see this common word? author_count = federalist_by_author_tokens[author].count(word) disputed_count = federalist_by_author_tokens[&quot;Disputed&quot;].count(word) # How often should we see it? expected_author_count = joint_count * author_share expected_disputed_count = joint_count * (1-author_share) # Add the word&#39;s contribution to the chi-squared statistic chisquared += ((author_count-expected_author_count) * (author_count-expected_author_count) / expected_author_count) chisquared += ((disputed_count-expected_disputed_count) * (disputed_count-expected_disputed_count) / expected_disputed_count) print(&quot;The Chi-squared statistic for candidate&quot;, author, &quot;is&quot;, chisquared) . The Chi-squared statistic for candidate Hamilton is 3434.6850314768426 The Chi-squared statistic for candidate Madison is 1907.5992915766838 . The chi-squared distance between the Disputed and Hamilton corpora is significantly greater than the distance between the Madison and Disputed corpora, as shown by the above results. This is a strong indication that, if a single author is responsible for the 12 papers in the Disputed corpus, Madison rather than Hamilton is that author. . However, chi-squared is still a crude method. For one thing, words that appear frequently tend to carry a disproportionate amount of weight in the final calculation. This is sometimes acceptable; other times, subtle differences in style represented by the ways in which authors use more unusual words will go unnoticed. . John Burrows&#8217; Delta Method (Advanced) . Like Kilgariff&#39;s chi-squared, Burrows&#39; Delta is a measure of the “distance” between a text whose authorship we want to ascertain and some other corpus. Unlike chi-squared, however, the Delta Method is designed to compare an anonymous text (or set of texts) to many different authors&#39; signatures at the same time. . We have a database of authors with some of their texts and a sample text of unknown authorship. | We want to order the authors by likelihood of authorship. | Therefore, measure the difference of a sample text and an author by a single value – Delta. | The most likely author will be the one with the least delta. | . Another advantage is that the Delta Method gives equal weight to every feature that it measures, thus avoiding the problem of common words overwhelming the results, which was an issue with chi-squared tests. . The following lines summarized Burrows’ original algorithm, and we are going to apply each step into the real case study. . Assemble a large corpus made up of texts written by an arbitrary number of authors, and find the n most frequent words in the corpus to use as features. . | For each of these n features, calculate the share of each of the x authors’ subcorpora represented by this feature, as a percentage of the total number of words. As an example, the word “the” may represent 4.72% of the words in Author A’s subcorpus. . | Then, calculate the mean and the standard deviation of these x values and use them as the offical mean and standard deviation for this feature over the whole corpus. In other words, we use sample mean instead of calculating a single value representing the share of the entire corpus represented by each word. This is because we want to avoid a larger subcorpus, like Hamilton’s in our case, over-influencing the results in its favor and defining the corpus norm in such a way that everything would be expected to look like it. . | For each of the n features and x subcorpora, calculate a z-score standardizing how far away from the corpus norm the usage of this particular feature in this particular subcorpus happens to be. To do this, subtract the “sample mean” for the feature from the feature’s frequency in the subcorpus and divide the result by the feature’s standard deviation. Figure below shows the z-score equation for feature ‘i’, where C(i) represents the observed frequency, the greek letter mu represents the mean of means, and the greek letter sigma, the standard deviation. . | . Then, calculate the same z-scores for each feature in the text for which we want to determine authorship. . | Finally, calculate a delta score comparing the anonymous paper with each candidate’s subcorpus. To do this, take the average of the absolute values of the differences between the z-scores for each feature between the anonymous paper and the candidate’s subcorpus. (Read that twice!) This gives equal weight to each feature, no matter how often the words occur in the texts; otherwise, the top 3 or 4 features would overwhelm everything else. Figure 8 shows the equation for Delta, where Z(c,i) is the z-score for feature ‘i’ in candidate ‘c’, and Z(t,i) is the z-score for feature ‘i’ in the test case. . | The “winning” candidate is the author for whom the delta score between the author’s subcorpus and the test case is the lowest. . Real Case Study . We&#39;ll utilize Federalist 64 as a test scenario. Because John Burrows&#39; Delta Method works with an arbitrary number of candidate authors (Burrows&#39; original paper uses approximately 25), we will compare Federalist 64&#39;s stylistic signature to those of five corpora: Hamilton&#39;s papers, Madison&#39;s papers, Jay&#39;s other papers, papers co-written by Madison and Hamilton, and papers disputed by Hamilton and Madison. We anticipate the Delta Technique to inform us that Jay is the most probable author; any other conclusion would bring the method, historiography, or both into doubt. . 1. Feature Selection . Let&#39;s group all of the subcorpora into a single corpus so that Delta can determine a &quot;standard&quot; to operate with. Then, choose a few words to serve as features. Remember that we used 500 words to calculate Kilgariff&#39;s chi-squared; this time, we&#39;ll use a smaller group of 30 words as our features, with the majority, if not all, of them being function words and common verbs. The output is a sample of the most frequent words with their frequency occurence. . authors = (&quot;Hamilton&quot;, &quot;Madison&quot;, &quot;Jay&quot;, &quot;Disputed&quot;, &quot;Shared&quot;) # Convert papers to lowercase to count all tokens of the same word together # regardless of case for author in authors: federalist_by_author_tokens[author] = ( [tok.lower() for tok in federalist_by_author_tokens[author]]) # Combine every paper except our test case into a single corpus whole_corpus = [] for author in authors: whole_corpus += federalist_by_author_tokens[author] # Get a frequency distribution whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(30)) whole_corpus_freq_dist[ :10 ] . [(&#39;the&#39;, 17846), (&#39;of&#39;, 11796), (&#39;to&#39;, 7012), (&#39;and&#39;, 5016), (&#39;in&#39;, 4408), (&#39;a&#39;, 3967), (&#39;be&#39;, 3770), (&#39;that&#39;, 2747), (&#39;it&#39;, 2520), (&#39;is&#39;, 2178)] . 2. Calculating features for each subcorpus . Consider the frequency of each characteristic in each candidate&#39;s subcorpus as a percentage of the total number of tokens in the subcorpus. We&#39;ll compute these values and store them in a dictionary of dictionaries. . features = [word for word,freq in whole_corpus_freq_dist] feature_freqs = {} for author in authors: # A dictionary for each candidate&#39;s features feature_freqs[author] = {} # A helper value containing the number of tokens in the author&#39;s subcorpus overall = len(federalist_by_author_tokens[author]) # Calculate each feature&#39;s presence in the subcorpus for feature in features: presence = federalist_by_author_tokens[author].count(feature) feature_freqs[author][feature] = presence / overall . 3. Calculating feature averages and standard . We may calculate a &quot;mean of means&quot; and a standard deviation for each feature based on the feature frequencies for all four subcorpora that we just computed. These values will be saved in another &quot;dictionary of dictionaries.&quot; . import math # The data structure into which we will be storing the &quot;corpus standard&quot; statistics corpus_features = {} # For each feature... for feature in features: # Create a sub-dictionary that will contain the feature&#39;s mean # and standard deviation corpus_features[feature] = {} # Calculate the mean of the frequencies expressed in the subcorpora feature_average = 0 for author in authors: feature_average += feature_freqs[author][feature] feature_average /= len(authors) corpus_features[feature][&quot;Mean&quot;] = feature_average # Calculate the standard deviation using the basic formula for a sample feature_stdev = 0 for author in authors: diff = feature_freqs[author][feature] - corpus_features[feature][&quot;Mean&quot;] feature_stdev += diff*diff feature_stdev /= (len(authors) - 1) feature_stdev = math.sqrt(feature_stdev) corpus_features[feature][&quot;StdDev&quot;] = feature_stdev . 4. Calculating z-scores . Following that, we convert the observed feature frequencies in the five candidates&#39; subcorpora into z-scores that describe how much these observations deviate from the &quot;corpus norm.&quot; Nothing special here: we just apply the z-score definition to each characteristic and save the results in yet another two-dimensional array. . feature_zscores = {} for author in authors: feature_zscores[author] = {} for feature in features: # Z-score definition = (value - mean) / stddev # We use intermediate variables to make the code easier to read feature_val = feature_freqs[author][feature] feature_mean = corpus_features[feature][&quot;Mean&quot;] feature_stdev = corpus_features[feature][&quot;StdDev&quot;] feature_zscores[author][feature] = ((feature_val-feature_mean) / feature_stdev) . 5. Calculating features and z-scores for our test case . Following that, we must contrast Federalist 64 with the corpus. The following code piece, which effectively summarizes what we&#39;ve done so far, counts the frequency of each of our 30 characteristics in Federalist 64 and computes z-scores accordingly . testcase_tokens = nltk.word_tokenize(federalist_by_author[&quot;TestCase&quot;]) # Filter out punctuation and lowercase the tokens testcase_tokens = [token.lower() for token in testcase_tokens if any(c.isalpha() for c in token)] # Calculate the test case&#39;s features overall = len(testcase_tokens) testcase_freqs = {} for feature in features: presence = testcase_tokens.count(feature) testcase_freqs[feature] = presence / overall # Calculate the test case&#39;s feature z-scores testcase_zscores = {} for feature in features: feature_val = testcase_freqs[feature] feature_mean = corpus_features[feature][&quot;Mean&quot;] feature_stdev = corpus_features[feature][&quot;StdDev&quot;] testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev print(&quot;Test case z-score for feature&quot;, feature, &quot;is&quot;, testcase_zscores[feature]) . Test case z-score for feature the is -0.7692828380408238 Test case z-score for feature of is -1.8167784558461264 Test case z-score for feature to is 1.032705844508835 Test case z-score for feature and is 1.0268752924746058 Test case z-score for feature in is 0.6085448502160903 Test case z-score for feature a is -0.9341289591084886 Test case z-score for feature be is 1.0279650702511498 Test case z-score for feature that is 1.7937385529385421 Test case z-score for feature it is -0.13459361853279056 Test case z-score for feature is is -0.9061542167373068 Test case z-score for feature which is -2.059010144513673 Test case z-score for feature by is 1.1019070073046568 Test case z-score for feature as is 4.9191578751913125 Test case z-score for feature this is -1.2530109173143964 Test case z-score for feature not is 0.7961288069167818 Test case z-score for feature would is -0.8319738801201663 Test case z-score for feature for is -1.1476926111838774 Test case z-score for feature have is 2.3422900648666367 Test case z-score for feature will is 1.504662365589896 Test case z-score for feature or is -0.2501345229633525 Test case z-score for feature from is -0.6289399837959637 Test case z-score for feature their is 0.7672330564777179 Test case z-score for feature with is -0.17724104821381542 Test case z-score for feature are is 7.827980222511478 Test case z-score for feature on is -0.046634869341845644 Test case z-score for feature an is -0.8211121241183916 Test case z-score for feature they is 4.269967908526837 Test case z-score for feature states is -1.1664427990875064 Test case z-score for feature government is -2.043489634686769 Test case z-score for feature may is 0.9954644116572955 . 6. Calculating Delta . Finally, we utilize Burrows&#39; Delta calculation to get a single score comparing Federalist 64 to each of the five &quot;possible writers.&quot; Reminder: the lower the Delta score, the more similar the stylometric signature of Federalist 64 is to the candidate&#39;s. . for author in authors: delta = 0 for feature in features: delta += math.fabs((testcase_zscores[feature] - feature_zscores[author][feature])) delta /= len(features) print( &quot;Delta score for candidate&quot;, author, &quot;is&quot;, delta ) . Delta score for candidate Hamilton is 1.768470453004334 Delta score for candidate Madison is 1.6089724119682816 Delta score for candidate Jay is 1.5345768956569326 Delta score for candidate Disputed is 1.5371768107570636 Delta score for candidate Shared is 1.846113566619675 .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/08/Stylometry.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/08/Stylometry.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 2",
            "content": "Required Packages . wordcloud, geopandas, nbformat, seaborn, scikit-learn . Now let&#39;s get started! . First thing first, you need to load all the necessary libraries: . import pandas as pd from matplotlib import pyplot as plt import numpy as np from wordcloud import WordCloud from wordcloud import STOPWORDS import re import plotly.graph_objects as go import seaborn as sns . Data Cleaning . Following the previous blog, the raw data we scraped from Amazon look like below. . Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my github notebooks. . Below is data after cleaning. It contains 6 columns and more than 500 rows. . . EDA . Below are the questions I curioused about, and the result generated by doing data analysis. . Which rating (1-5) got the most and least? . | Which country are they targeting? . | Which month people prefer to give a higher rating? . | Which month people leave commons the most? . | . What are the useful words that people mentioned in the reviews? | . . Sentiment Analysis (Method 1) . What is sentiment analysis? . Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as &quot;somewhat positive&quot; and &quot;somewhat negative.&quot; . In this specific case, we catogrize 4 and 5 stars to the positive group and 1 &amp; 2 stars to the negative gorup. . . Below are the most frequently words in reviews from positive group and negative group respectively. . Positive review . Negative review . Build up the first model . Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative. . Because this is a classification task, we will train a simple logistic regression model. . Clean Data First, we create a new function to remove all punctuations from the data for later use. | . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final . Split the Dataframe | . Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews. . . df[&#39;random_number&#39;] = np.random.randn(len(index)) train = df[df[&#39;random_number&#39;] &lt;= 0.8] test = df[df[&#39;random_number&#39;] &gt; 0.8] . Create a bag of words | . Here I would like to introduce a new package. . Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. . In this example, we are going to use sklearn.feature_extraction.text.CountVectorizer to convert a collection of text documents to a matrix of token counts. . The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text. . train_matrix = vectorizer.fit_transform(train[&#39;title&#39;]) test_matrix = vectorizer.transform(test[&#39;title&#39;]) . Import Logistic Regression | . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . Split target and independent variables | . X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] . Fit model on data | . lr.fit(X_train,y_train) . Make predictionsa | . predictions = lr.predict(X_test) . The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa. . Testing . Now, we can test the accuracy of our model! . from sklearn.metrics import confusion_matrix, classification_report new = np.asarray(y_test) confusion_matrix(predictions,y_test) print(classification_report(predictions,y_test)) . . The accuracy is as high as 89%! . Sentiment Analysis (Method 2) . In this process, you will learn how to build your own sentiment analysis classifier using Python and understand the basics of NLP (natural language processing). First, let&#39;s try to use a quick and dirty method to utilize the Naive Bayes classifier to predict the sentiments of Amazon product review. . Based on the application&#39;s requirements, we should first put each review in a txt file and catogorize them as negative or positive review in different folder. . neg = df[df[&quot;sentiment&quot;] == -1].review #Reset the index neg.index = range(len(neg.index)) ## Write each DataFrame to separate txt for i in range(len(neg)): data = neg[i] with open(str(i) + &quot;.txt&quot;,&quot;w&quot;) as file: file.write(data + &quot; n&quot;) . Next, we sort the order of the official data and remove all the content. In other words, we only keep the file name. . import os import pandas as pd #Get file names file_names = os.listdir(&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg&#39;) #Convert pandas neg_df = pd.DataFrame (file_names, columns = [&#39;file_name&#39;]) #split to sort neg_df[[&#39;number&#39;,&#39;id&#39;]] = neg_df.file_name.apply( lambda x: pd.Series(str(x).split(&quot;_&quot;))) #change the number to be the index neg_df_index = neg_df.set_index(&#39;number&#39;) neg_org = neg_df_index.sort_index(ascending=True) #del neg[&quot;id&quot;] neg_org.reset_index(inplace=True) neg_org = neg_org.drop([0], axis=0).reset_index(drop=True) neg_names = neg_org[&#39;file_name&#39;] for file_name in neg_names: t = open(f&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{file_name}&#39;, &#39;w&#39;) t.write(&quot;&quot;) t.close() . Next, we insert the content of amazon review to the official files with their original file names. . file_names = os.listdir(&#39;/Users/zeyu/Desktop/DS/neg&#39;) #Convert pandas pos_df = pd.DataFrame (file_names, columns = [&#39;file_name&#39;]) pos_names = pos_df[&#39;file_name&#39;] for index, file_name in enumerate(pos_names): try: t = open(f&#39;/Users/zeyu/Desktop/DS/neg/{file_name}&#39;, &#39;r&#39;) # t.write(&quot;&quot;) t_val = ascii(t.read()) t.close() writefname = pos_names_org[index] t = open(f&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{writefname}&#39;, &#39;w&#39;) t.write(t_val) t.close() except: print(f&#39;{index} Reading/writing Error&#39;) . Eventually, we can just run these few lines to predict the sentiments of Amazon product review. . import nltk from nltk.corpus import movie_reviews import random documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)] #All words, not unique. random.shuffle(documents) . all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) #Only show first 2000. word_features = list(all_words)[:2000] def document_features(document): document_words = set(document) features = {} for word in word_features: features[&#39;contains({})&#39;.format(word)] = (word in document_words) return features . featuresets = [(document_features(d), c) for (d,c) in documents] train_set, test_set = featuresets[100:], featuresets[:100] classifier = nltk.NaiveBayesClassifier.train(train_set) print(nltk.classify.accuracy(classifier, test_set)) . classifier.show_most_informative_features(5) .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping2.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping2.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 1",
            "content": "Let&#39;s get started! . Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. . In this blog, we&#39;ll look at scraping Amazon reviews with Python&#39;s Scrapy. Scrapy is a python web crawling framework, and web scraping is a simple way of collecting data from many websites. Web scraping enables users to manage data for their specific needs, such as online merchandising, price monitoring, and marketing decision-making. . If you&#39;re wondering whether or whether this procedure is legal, you can discover the answer here. . Before we get into scanning Amazon for product reviews, let&#39;s take a look at some of the reasons why you might want to scrape Amazon reviews in the first place. . What exactly is Scrapy? . Scrapy is a web crawling framework that allows a developer to write code that defines how a specific site (or a group of websites) will be scraped. The most important feature is that it is built on Twisted, an asynchronous networking library, which significantly improves spider performance. . Why is it needed to scrape Amazon reviews? . Product reviews were subjected to a sentiment analysis. The reviews scraped from Amazon products can be subjected to sentiment analysis. This type of research aids in determining a user&#39;s emotional response to a certain product. This can assist sellers, as well as other potential customers, in gaining a better understanding of public opinion about the goods. . | Dropshipping sales optimization Dropshipping is a business model that allows a company to operate without an inventory or a depository for product storage. You can utilize web scraping to acquire product pricing, user opinions, consumer wants, and trend information. . | Web scraping is used to keep track of a brand&#39;s online reputation. It is tough for major corporations to keep track of their product reputation. Web scraping can aid in the extraction of pertinent review data, which can then be fed into various analysis tools to gauge user attitude toward the company. . | . The overall view . Examining the webpage&#39;s HTML structure Scraping is the process of identifying patterns in web pages and extracting them. Before we begin writing a scraper, we must first understand the HTML structure of the target web page and identify patterns in it. The pattern is related to the repetitive use of classes, ids, and other HTML elements. . | Python scrapy parser implementation We work on the coded implementation in Python after analyzing the structure of the target web page. Scrapy parser&#39;s responsibility is to visit the targeted web page and extract the information according to the rules specified. . | Information Gathering and Storage The parser can output the results in any format you want, including CSV and JSON. This is the final output where your scraped data is stored. . | . Shure MV7 . The product review we are going to scrap is the lastest dual USB;XLR microphone called Shure MV7. . . Shure is probabily the most well-known micphone company that Michael Jackson trusted. . . Examining the webpage&#39;s HTML structure . At the very beginning, let&#39;s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews In actualality, the url can be shorten as below. https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 THis looks clearer now. We go the the Amazon&#39;s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer &quot;1&quot; on the url represents the first page of the review pages. We can change it to 2, 3, ... until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute &quot;review-rating,&quot; and review texts are represented by the class &quot;review-text.&quot; All we have to do now is use our Scrapy parser to pick up these patterns. . Examining the webpage&#39;s HTML structure . At the very beginning, let&#39;s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews In actualality, the url can be shorten as below. https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 THis looks clearer now. We go the the Amazon&#39;s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer &quot;1&quot; on the url represents the first page of the review pages. We can change it to 2, 3, ... until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute &quot;review-rating,&quot; and review texts are represented by the class &quot;review-text.&quot; All we have to do now is use our Scrapy parser to pick up these patterns. . Python scrapy parser implementation . Install &quot;scrapy&quot; package . First, let&#39;s install the scrapy package by enter the line below. . pip install scrapy . Then to create a scrapy project use following command. . scrapy startproject amazon_reviews_scraping . After entering the code, you should see these two. One is a folder that contains your scrapy code, and the other is your spacy configuration file. Spacy configuration while helps in running and deploying the Scrapy project on a server. . We&#39;ll need to creat a spider once we have a project in place. A spider is a piece of Python code that controls how a web page is scraped. It is the main component that crawls various web pages and extracts content from them. In our case, this will be the code chuck that will visit Amazon and scrape Amazon reviews. You can use the following command to make a spider. . scrapy genspider amazon_review your-link-here . The structure is like the image below. . . Files description . items.py Items are containers that will be loaded with the scraped data. | . Middleware.py The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders. | . Pipelines.py After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class. | . settings.py It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. | . spiders folder The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. | . Here is the deeper explaination about each files. . Defining Scrapy Parser in Python . Now we go to the spider folder and open the file reviews.py. In this process, we create two variables which are reviews_url and asin_list for convience later. Then we create a new function to request the website. . reviews_url = &#39;https://www.amazon.com/product-reviews/{}&#39; asin_list = [&#39;B08G7RG9ML&#39;] class ReviewsSpider(scrapy.Spider): name = &#39;reviews&#39; def start_requests(self): for asin in asin_list: url = reviews_url.format(asin) yield scrapy.Request(url) . Then we download another package called scraper-helper to make the process even easier. . pip install scraper-helper . Now we go to folder Amazon_reviews_scraping and open settings.py. We delete everything but these lines. . import scraper_helper as sh BOT_NAME = &#39;Amazon_reviews_scraping&#39; SPIDER_MODULES = [&#39;Amazon_reviews_scraping.spiders&#39;] NEWSPIDER_MODULE = &#39;Amazon_reviews_scraping.spiders&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False DEFAULT_REQUEST_HEADERS = sh.get_dict( &quot;&quot;&quot; accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9, accept-encoding: deflate, accept-language: en-US,en;q=0.9, sec-ch-ua: &quot;Chromium&quot;;v=&quot;88&quot;, &quot;Google Chrome&quot;;v=&quot;88&quot;, &quot;;Not A Brand&quot;;v=&quot;99&quot;, sec-ch-ua-mobile: ?0, sec-fetch-dest: document, sec-fetch-mode: navigate, sec-fetch-site: none, sec-fetch-user: ?1, upgrade-insecure-requests: 1, user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36 &quot;&quot;&quot; ) . We use the new import library method &quot;get_dict&quot; to convert the string lines from comments to dictions format. You can simply copy and paste this piece because this process required some html knowledge. After this, we fit the request header and can successfully connect to the website. . The next step is to go back the review.py and finish the function &quot;parse&quot;. We open the inspect tool and can easily find that each star rating, data to review, and comments about the preduct has their own type unique headers, such as &quot;[data-hook=&quot;review-star-rating&quot;]&quot; for star rating. We find each unique header and get each type of information we need. . import scrapy reviews_url = &#39;https://www.amazon.com/product-reviews/{}&#39; asin_list = [&#39;B08G7RG9ML&#39;] class ReviewsSpider(scrapy.Spider): name = &#39;reviews&#39; def start_requests(self): for asin in asin_list: url = reviews_url.format(asin) yield scrapy.Request(url) def parse(self, response): #print(&#39;Im in parse&#39;) for review in response.css(&#39;[data-hook=&quot;review&quot;]&#39;): item = { &#39;name&#39;: review.css(&#39;.a-profile-name::text&#39;).get(), &#39;stars1&#39;: review.css (&#39;[data-hook=&quot;review-star-rating&quot;] ::text&#39;) .get (), &#39;stars2&#39;: review.css (&#39;[data-hook=&quot;cmps-review-star-rating&quot;] ::text&#39;) .get (), &#39;date&#39;: review.css (&#39;[data-hook=&quot;review-date&quot;] ::text&#39;) .get (), &#39;title&#39;: review.css (&#39;[data-hook=&quot;review-title&quot;] span ::text&#39;) .get (), &#39;review&#39;: review.xpath(&#39;normalize-space(.//*[@data-hook=&quot;review-body&quot;])&#39;).get() } yield item next_page = response.xpath(&#39;//a[text ()=&quot;Next page&quot;]/@href&#39;).get() if next_page: yield scrapy.Request(response.urljoin(next_page)) . Of course, after examining the css file, we can create a few lines to make sure after we scrape the date, it jump to the next page automatically and make sure we can collect enought data we need. . next_page = response.xpath(&#39;//a[text ()=&quot;Next page&quot;]/@href&#39;).get() if next_page: yield scrapy.Request(response.urljoin(next_page)) . Eventually, we run the code below by using terminal. . scrapy crawl reviews -o new_file_name.csv . This will generate a new csv file to help us to store the data we request from Amazon. . Conclusion . We did it! . Even though the whole process looks easy, but there are still some unexpected things that are warting for you to explore. For example, for star rating, different countries have different headers like below. . United States: . [data-hook=&quot;review-star-rating&quot;] . Other countries:&gt; [data-hook=&quot;cmps-review-star-rating&quot;] . Another difficulty in scraping Amazon reviews is that Amazon tends to block IPs if you scrape Amazon frequently. This can be an impediment to your work. . In such cases, make sure to rotate your IP addresses on a regular basis and make fewer requests to the Amazon server to avoid being blocked. More information can be found here. . In the next blog, part 2, we are going to clean up the data and dig in the sentiment analysis. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping1.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping1.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Introducing spaCy For NLP",
            "content": "First, what is spacy? . SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python that allows you to perform extensive natural language processing analysis and develop models that may be used to support document analysis, chatbot capabilities, and other types of text analysis. . This blog should cover a few features you need to know about spaCy, whether you&#39;re new to it or just want to brush up on some NLP basics and implementation specifics. . It&#39;s become one of the most extensively used natural language libraries in Python for corporate use cases, and it has a sizable community—and with it, a lot of support for commercializing research breakthroughs as this field evolves rapidly. . After downloading a list of packages, we can load spaCy and run some code like below. That nlp variable, which is loaded with the en_core_web_sm small model for English. . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . Next, let&#39;s run a small &quot;document&quot; through the natural language parser: . doc = nlp(&quot;He went to play basketball&quot;) . nlp.pipe_names shows spaCy’s processing pipeline. The NLP pipeline has multiple components, such as tokenizer, tagger, parser, ner, etc. So, the input text string has to go through all these components before we can work on it. . nlp.pipe_names . [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;attribute_ruler&#39;, &#39;lemmatizer&#39;, &#39;ner&#39;] . Just in case you wish to disable the pipeline components and keep only the tokenizer up and running, then you can use the code nlp.disable_pipes to disable the pipeline components: . nlp.disable_pipes(&#39;tagger&#39;, &#39;parser&#39;) nlp.pipe_names . [&#39;tok2vec&#39;, &#39;attribute_ruler&#39;, &#39;lemmatizer&#39;, &#39;ner&#39;] . In the following section, you will learn to perform various NLP tasks using spaCy. We will start off with the popular NLP tasks of Part-of-Speech Tagging, Dependency Parsing, and Named Entity Recognition. . 1. Part-of-Speech (POS) Tagging using spaCy . In English, nouns, pronouns, adjectives, verbs, and adverbs are some of the most prevalent parts of speech. The task of automatically assigning POS tags to all the words in a sentence is known as POS tagging. It&#39;s useful for a variety of downstream NLP applications, including feature engineering, language comprehension, and information extraction. . for token in doc: # Print the token and its part-of-speech tag print(token.text, &quot;--&gt;&quot;, token.pos_) . He --&gt; PRON went --&gt; VERB to --&gt; PART play --&gt; VERB basketball --&gt; NOUN . 2. Dependency Parsing using spaCy . Every sentence has a grammatical structure, which may be extracted with the assistance of dependency parsing. It can alternatively be viewed as a directed graph, with nodes corresponding to the words in the sentence and edges between nodes corresponding to the word dependencies. Performing dependency parsing in spaCy is simple as well. We&#39;ll use the same document we did for POS labeling here: . for token in doc: print(token.text, &quot;--&gt;&quot;, token.dep_) . He --&gt; nsubj went --&gt; ROOT to --&gt; aux play --&gt; advcl basketball --&gt; dobj . 3. Named Entity Recognition using spaCy . What entities are. Entities are words or collections of words that represent information about everyday objects like people, places, and organizations. These things have official names. Like the example below, Today is a date. Chris is assumed as a person. . doc = nlp(&quot;Today is March 21, and Chris has already spent a few hours on this project in the United States.&quot;) for ent in doc.ents: print(ent.text, ent.label_) . Today DATE March 21 DATE Chris PERSON a few hours TIME the United States GPE . /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically &#39;tagger&#39;+&#39;attribute_ruler&#39; or &#39;morphologizer&#39;. warnings.warn(Warnings.W108) . 4. Rule-Based Matching using spaCy . It&#39;s hard to explain the meaning of Rule-Based matching using spacy; however, the output of the following code demenstrate the meaning cristal clear. With this spaCy matcher, you can find words and phrases in the text using user-defined rules. . So, in the code below: . First, we import the spaCy matcher | After that, we initialize the matcher object with the default spaCy vocabulary | Then, we pass the input in an NLP object as usual | In the next step, we define the rule/pattern for what we want to extract from the text. | . Let&#39;s imagine we&#39;re looking for the term &quot;lemon water&quot; in a text. As a result, our goal is for the matcher to be able to discover this pattern in the text whenever &quot;lemon&quot; is followed by the word &quot;water.&quot; That&#39;s exactly what we did in the code above when defining the pattern. Finally, the defined rule is applied to the matcher object. . from spacy.matcher import Matcher nlp = spacy.load(&quot;en_core_web_sm&quot;) # Initialize the matcher with the spaCy vocabulary matcher = Matcher(nlp.vocab) doc = nlp(&quot;Some people start their day with lemon water&quot;) # Define rule pattern = [{&#39;TEXT&#39;: &#39;lemon&#39;}, {&#39;TEXT&#39;: &#39;water&#39;}] # Add rule matcher.add(&#39;rule_1&#39;,[pattern]) print(matcher(doc)) matches = matcher(doc) matches # Extract matched text for match_id, start, end in matches: # Get the matched span matched_span = doc[start:end] print(matched_span.text) . [(7604275899133490726, 6, 8)] lemon water . The output above has three elements. The first element, ‘7604275899133490726’, is the match ID. The second and third elements are the positions of the matched tokens. . Let&#39;s have a look at another application of the spaCy matcher. Consider the following two sentences: . You can read this book | I will book my ticket | . We are now interested in determining whether or not a sentence contains the word &quot;book.&quot; It appears to be quite simple, doesn&#39;t it? But here&#39;s the catch: we can only find the word &quot;book&quot; if it was used as a noun in the sentence. . The word &quot;book&quot; was used as a noun in the first sentence and as a verb in the second sentence. As a result, the spaCy matcher should only be able to extract the pattern from the first sentence. Let&#39;s put it to the test: . doc1 = nlp(&quot;You read this book&quot;) doc2 = nlp(&quot;I will book my ticket&quot;) #Text and POS are required pattern = [{&#39;TEXT&#39;: &#39;book&#39;, &#39;POS&#39;: &#39;NOUN&#39;}] # Initialize the matcher with the shared vocab matcher = Matcher(nlp.vocab) matcher.add(&#39;rule_2&#39;, [pattern]) print(&quot;doc1: &quot;, matcher(doc1),&quot; ndoc2: &quot;, matcher(doc2)) . doc1: [(375134486054924901, 3, 4)] doc2: [] . This was a quick overview to give you an idea of what spaCy can do. Trust me, you&#39;ll find yourself using spaCy frequently for NLP tasks. I encourage you to experiment with the code, download a dataset from DataHack, and try your hand at it with spaCy. . Now, let&#8217;s get our hands dirty with spaCy. . First, let&#39;s play around. . That nlp variable is now your gateway to all things spaCy and loaded with the en_core_web_sm small model for English. Next, let&#39;s run a small &quot;document&quot; through the natural language parser. . We started by making a doc out of the text, which is a container for a document and all of its annotations. Then we went through the document iteratively to see what spaCy had parsed. . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) text = &quot;The rain in Spain falls mainly on the plain.&quot; doc = nlp(text) # stoplist for token in doc: print(token.text, token.lemma_, token.pos_, token.is_stop) . The the DET True rain rain NOUN False in in ADP True Spain Spain PROPN False falls fall VERB False mainly mainly ADV False on on ADP True the the DET True plain plain NOUN False . . PUNCT False . It&#39;s good, but there&#39;s a lot of information and it&#39;s a little difficult to read. Let&#39;s make the spaCy parse of that sentence into a pandas dataframe: . import pandas as pd cols = (&quot;text&quot;, &quot;lemma&quot;, &quot;POS&quot;, &quot;explain&quot;, &quot;stopword&quot;) rows = [] for t in doc: row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop] rows.append(row) df = pd.DataFrame(rows, columns=cols) df . text lemma POS explain stopword . 0 The | the | DET | determiner | True | . 1 rain | rain | NOUN | noun | False | . 2 in | in | ADP | adposition | True | . 3 Spain | Spain | PROPN | proper noun | False | . 4 falls | fall | VERB | verb | False | . 5 mainly | mainly | ADV | adverb | False | . 6 on | on | ADP | adposition | True | . 7 the | the | DET | determiner | True | . 8 plain | plain | NOUN | noun | False | . 9 . | . | PUNCT | punctuation | False | . Next, let&#39;s use the displaCy library to visualize the parse tree for the sentence. If you are familier with parse tree, a parse tree, also known as a parsing tree, derivation tree, or concrete syntax tree, is a rooted tree that represents the syntactic structure of a string according to some context-free grammar. . from spacy import displacy displacy.render(doc, style=&quot;dep&quot;) . The DET rain NOUN in ADP Spain PROPN falls VERB mainly ADV on ADP the DET plain. NOUN det nsubj prep pobj advmod prep det pobj However, the this only work on a single sentence. What if we have more sentences like a novel? There are features for sentence boundary detection (SBD)—also known as sentence segmentation—based on the builtin/default sentencizer: . text = &quot;We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in.Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.&quot; doc = nlp(text) for sent in doc.sents: print(&quot;&gt;&quot;, sent) . &gt; We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. &gt; I fell in. &gt; Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. &gt; The gorillas just went wild. . When spaCy generates a document, it employs a non-destructive tokenization principle, which means that the tokens, sentences, and so on are simply indexes into a large array. In other words, they do not cut up the text stream into small chunks. As a result, each sentence is a span with a start and end index into the document array: . for sent in doc.sents: print(&quot;&gt;&quot;, sent.start, sent.end) . &gt; 0 25 &gt; 25 29 &gt; 29 48 &gt; 48 54 . We can index into the document array to pull out the tokens for one sentence: . doc[48:54] . The gorillas just went wild. . Or simply index into a specific token, such as the verb went in the last sentence: . token = doc[51] print(token.text, token.lemma_, token.pos_) . went go VERB . At this point, we can parse a document, segment that document into sentences, then look at annotations about the tokens in each sentence. That&#39;s a good start. . Acquiring Text . Using the internet as a quick source is one option for us to acquire texts. Of course, we get HTML when we download web pages and must subsequently extract text from them. For this, Beautiful Soup is a popular package. . import sys import warnings warnings.filterwarnings(&quot;ignore&quot;) . In the following function get_text() we&#39;ll parse the HTML to find all of the tags, then extract the text for those: . from bs4 import BeautifulSoup import requests import traceback def get_text (url): buf = [] try: soup = BeautifulSoup(requests.get(url).text, &quot;html.parser&quot;) for p in soup.find_all(&quot;p&quot;): buf.append(p.get_text()) return &quot;&quot;.join(buf) except: print(traceback.format_exc()) sys.exit(-1) . Now let&#39;s grab some text from online sources. We can compare open-source licenses hosted on the Open Source Initiative site: . lic = {} lic[&quot;mit&quot;] = nlp(get_text(&quot;https://opensource.org/licenses/MIT&quot;)) lic[&quot;asl&quot;] = nlp(get_text(&quot;https://opensource.org/licenses/Apache-20&quot;)) lic[&quot;bsd&quot;] = nlp(get_text(&quot;https://opensource.org/licenses/BSD-3-Clause&quot;)) for sent in lic[&quot;bsd&quot;].sents: print(&quot;&gt;&quot;, sent) . &gt; SPDX short identifier: BSD-3-Clause Note: This license has also been called the &#34;New BSD License&#34; or &#34;Modified BSD License&#34;. &gt; See also the 2-clause BSD License. &gt; Copyright &lt;YEAR&gt; &lt;COPYRIGHT HOLDER&gt;Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:1. &gt; Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.3. &gt; Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. &gt; THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND &gt; CONTRIBUTORS &gt; &#34;AS IS&#34; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. &gt; IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) &gt; HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.For over 20 years the Open Source Initiative (OSI) has worked to raise awareness and adoption of open source software, and build bridges between open source communities of practice. &gt; As a global non-profit, the OSI champions software freedom in society through education, collaboration, and infrastructure, stewarding the Open Source Definition (OSD), and preventing abuse of the ideals and ethos inherent to the open source movement. &gt; Open source software is made by many people and distributed under an OSD-compliant license which grants all the rights to use, study, change, and share the software in modified and unmodified form. &gt; Software freedom is essential to enabling community development of open source software. &gt; Sign-up for our newsletter!The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License. &gt; Opensource.org is not the author of any of the licenses reproduced on this site. &gt; Questions about the copyright in a license should be directed to the license steward. &gt; Hosting for Opensource.org is generously provided by DigitalOcean. &gt; Please see Terms of Service. &gt; For questions regarding the OSI website and contents please contact us. &gt;   . Text comparison is a common application for natural language work. With those open-source licenses, for example, we can download their text, parse it, and then compare similarity metrics among them: . pairs = [ [&quot;mit&quot;, &quot;asl&quot;], [&quot;asl&quot;, &quot;bsd&quot;], [&quot;bsd&quot;, &quot;mit&quot;]] for a, b in pairs: print(a, b, lic[a].similarity(lic[b])) . mit asl 0.7400606692738089 asl bsd 0.7629310829561079 bsd mit 0.976838684194463 . This is intriguing because the BSD and MIT licenses appear to be the most similar. They are, in fact, closely related. . Due to the OSI disclaimer in the footer, some extra text was included in each document—but this provides a reasonable approximation for comparing the licenses. . Natural Language Understanding . Now, let&#39;s look at some of the spaCy features for NLU. Given a parse of a document, we can extract the noun chunks, i.e., each of the noun phrases: . text = &quot;Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.&quot; doc = nlp(text) for chunk in doc.noun_chunks: print(chunk.text) . Steve Jobs Steve Wozniak Apple Computer January Cupertino California . Noun phrases in a sentence generally provide more information content as a simple filter used to reduce a long document into a more &quot;distilled&quot; representation. . This approach can be extended by identifying named entities within the text, i.e., proper nouns: . for ent in doc.ents: print(ent.text, ent.label_) . Steve Jobs PERSON Steve Wozniak PERSON Apple Computer ORG January 3, 1977 DATE Cupertino GPE California GPE . The displaCy library provides an excellent way to visualize named entities: . displacy.render(doc, style=&quot;ent&quot;) . Steve Jobs PERSON and Steve Wozniak PERSON incorporated Apple Computer ORG on January 3, 1977 DATE , in Cupertino GPE , California GPE . There&#39;s a spaCy integration for WordNet called spacy-wordnet by Daniel Vila Suero, an expert in natural language and knowledge graph work. . Then we&#39;ll load the WordNet data via NLTK (these things happen): . import nltk import ssl try: _create_unverified_https_context = ssl._create_unverified_context except AttributeError: pass else: ssl._create_default_https_context = _create_unverified_https_context nltk.download(&quot;wordnet&quot;) . [nltk_data] Downloading package wordnet to /Users/zeyu/nltk_data... [nltk_data] Package wordnet is already up-to-date! . True . It should be noted that spaCy operates as a &quot;pipeline&quot; and provides means for customizing parts of the pipeline in use. This is fantastic for enabling really interesting workflow integrations in data science work. We&#39;ll include the WordnetAnnotator from the spacy-wordnet project here: . from spacy_wordnet.wordnet_annotator import WordnetAnnotator print(&quot;before&quot;, nlp.pipe_names) #V3 difference if &quot;spacy_wordnet&quot; not in nlp.pipe_names: nlp.add_pipe(&quot;spacy_wordnet&quot;, after=&#39;tagger&#39;, config={&#39;lang&#39;: nlp.lang}) else: print(&quot;after&quot;, nlp.pipe_names) . before [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;attribute_ruler&#39;, &#39;lemmatizer&#39;, &#39;ner&#39;] . token = nlp(&quot;withdraw&quot;)[0] token._.wordnet.synsets() . [Synset(&#39;withdraw.v.01&#39;), Synset(&#39;retire.v.02&#39;), Synset(&#39;disengage.v.01&#39;), Synset(&#39;recall.v.07&#39;), Synset(&#39;swallow.v.05&#39;), Synset(&#39;seclude.v.01&#39;), Synset(&#39;adjourn.v.02&#39;), Synset(&#39;bow_out.v.02&#39;), Synset(&#39;withdraw.v.09&#39;), Synset(&#39;retire.v.08&#39;), Synset(&#39;retreat.v.04&#39;), Synset(&#39;remove.v.01&#39;)] . token._.wordnet.lemmas() . [Lemma(&#39;withdraw.v.01.withdraw&#39;), Lemma(&#39;withdraw.v.01.retreat&#39;), Lemma(&#39;withdraw.v.01.pull_away&#39;), Lemma(&#39;withdraw.v.01.draw_back&#39;), Lemma(&#39;withdraw.v.01.recede&#39;), Lemma(&#39;withdraw.v.01.pull_back&#39;), Lemma(&#39;withdraw.v.01.retire&#39;), Lemma(&#39;withdraw.v.01.move_back&#39;), Lemma(&#39;retire.v.02.retire&#39;), Lemma(&#39;retire.v.02.withdraw&#39;), Lemma(&#39;disengage.v.01.disengage&#39;), Lemma(&#39;disengage.v.01.withdraw&#39;), Lemma(&#39;recall.v.07.recall&#39;), Lemma(&#39;recall.v.07.call_in&#39;), Lemma(&#39;recall.v.07.call_back&#39;), Lemma(&#39;recall.v.07.withdraw&#39;), Lemma(&#39;swallow.v.05.swallow&#39;), Lemma(&#39;swallow.v.05.take_back&#39;), Lemma(&#39;swallow.v.05.unsay&#39;), Lemma(&#39;swallow.v.05.withdraw&#39;), Lemma(&#39;seclude.v.01.seclude&#39;), Lemma(&#39;seclude.v.01.sequester&#39;), Lemma(&#39;seclude.v.01.sequestrate&#39;), Lemma(&#39;seclude.v.01.withdraw&#39;), Lemma(&#39;adjourn.v.02.adjourn&#39;), Lemma(&#39;adjourn.v.02.withdraw&#39;), Lemma(&#39;adjourn.v.02.retire&#39;), Lemma(&#39;bow_out.v.02.bow_out&#39;), Lemma(&#39;bow_out.v.02.withdraw&#39;), Lemma(&#39;withdraw.v.09.withdraw&#39;), Lemma(&#39;withdraw.v.09.draw&#39;), Lemma(&#39;withdraw.v.09.take_out&#39;), Lemma(&#39;withdraw.v.09.draw_off&#39;), Lemma(&#39;retire.v.08.retire&#39;), Lemma(&#39;retire.v.08.withdraw&#39;), Lemma(&#39;retreat.v.04.retreat&#39;), Lemma(&#39;retreat.v.04.pull_back&#39;), Lemma(&#39;retreat.v.04.back_out&#39;), Lemma(&#39;retreat.v.04.back_away&#39;), Lemma(&#39;retreat.v.04.crawfish&#39;), Lemma(&#39;retreat.v.04.crawfish_out&#39;), Lemma(&#39;retreat.v.04.pull_in_one&#39;s_horns&#39;), Lemma(&#39;retreat.v.04.withdraw&#39;), Lemma(&#39;remove.v.01.remove&#39;), Lemma(&#39;remove.v.01.take&#39;), Lemma(&#39;remove.v.01.take_away&#39;), Lemma(&#39;remove.v.01.withdraw&#39;)] . token._.wordnet.wordnet_domains() . [&#39;astronomy&#39;, &#39;school&#39;, &#39;telegraphy&#39;, &#39;industry&#39;, &#39;psychology&#39;, &#39;ethnology&#39;, &#39;ethnology&#39;, &#39;administration&#39;, &#39;school&#39;, &#39;finance&#39;, &#39;economy&#39;, &#39;exchange&#39;, &#39;banking&#39;, &#39;commerce&#39;, &#39;medicine&#39;, &#39;ethnology&#39;, &#39;university&#39;, &#39;school&#39;, &#39;buildings&#39;, &#39;factotum&#39;, &#39;agriculture&#39;, &#39;mechanics&#39;, &#39;gastronomy&#39;, &#39;meteorology&#39;, &#39;physics&#39;, &#39;basketball&#39;, &#39;anatomy&#39;, &#39;skiing&#39;, &#39;nautical&#39;, &#39;engineering&#39;, &#39;racing&#39;, &#39;home&#39;, &#39;drawing&#39;, &#39;dentistry&#39;, &#39;ethnology&#39;, &#39;mathematics&#39;, &#39;furniture&#39;, &#39;animal_husbandry&#39;, &#39;industry&#39;, &#39;economy&#39;, &#39;body_care&#39;, &#39;chemistry&#39;, &#39;medicine&#39;, &#39;surgery&#39;, &#39;vehicles&#39;, &#39;transport&#39;, &#39;atomic_physic&#39;, &#39;archaeology&#39;, &#39;hydraulics&#39;, &#39;oceanography&#39;, &#39;golf&#39;, &#39;sculpture&#39;, &#39;earth&#39;, &#39;applied_science&#39;, &#39;artisanship&#39;] . Again, if you&#39;re working with knowledge graphs, those WordNet &quot;word sense&quot; links could be used in conjunction with graph algorithms to help identify the meanings of specific words. This technique, known as summarization, can also be used to create summaries for longer sections of text. It&#39;s beyond the scope of this tutorial, but it&#39;s an interesting application for natural language in the industry right now. . In the opposite direction, if you know ahead of time that a document is about a specific domain or set of topics, you can limit the meanings returned by WordNet. In the following example, we&#39;ll look at NLU results from the Finance and Banking sectors: . domains = [&quot;finance&quot;, &quot;banking&quot;] sentence = nlp(&quot;I want to withdraw 5,000 euros.&quot;) enriched_sent = [] for token in sentence: # get synsets within the desired domains synsets = token._.wordnet.wordnet_synsets_for_domain(domains) if synsets: lemmas_for_synset = [] for s in synsets: # get synset variants and add to the enriched sentence lemmas_for_synset.extend(s.lemma_names()) enriched_sent.append(&quot;({})&quot;.format(&quot;|&quot;.join(set(lemmas_for_synset)))) else: enriched_sent.append(token.text) print(&quot; &quot;.join(enriched_sent)) . I (privation|want|neediness|deprivation) (deficiency|deprivation|privation|lack|want|neediness) (deficiency|deprivation|privation|require|lack|want|need|neediness) to (withdraw|draw_off|draw|take_out) 5,000 euros . . That example may appear straightforward, but if you experiment with the domains list, you&#39;ll notice that the results exhibit a kind of combinatorial explosion when run without reasonable constraints. Consider a knowledge graph with millions of elements: you&#39;d want to limit searches wherever possible to avoid having each query take days/weeks/months/years to compute. . Text Comparison with spaCy and Scattertext . Sometimes the problems encountered when attempting to understand a text—or, better yet, attempting to understand a corpus (a dataset containing many related texts)—become so complex that they must first be visualized. Here&#39;s an interactive visualization to help you understand text: Jason Kessler&#39;s genius has resulted in scattertext. . Let us examine text data from party conventions held during the 2012 US Presidential elections. Please keep in mind that this cell may take a few minutes to run, but the results of all that number-crunching are well worth the wait. . import scattertext as st if &quot;merge_entities&quot; not in nlp.pipe_names: nlp.add_pipe(&#39;merge_entities&#39;) if &quot;merge_noun_chunks&quot; not in nlp.pipe_names: nlp.add_pipe(&#39;merge_noun_chunks&#39;) convention_df = st.SampleCorpora.ConventionData2012.get_data() corpus = st.CorpusFromPandas(convention_df, category_col=&quot;party&quot;, text_col=&quot;text&quot;, nlp=nlp).build() . convention_df . party text speaker . 0 democrat | Thank you. Thank you. Thank you. Thank you so ... | BARACK OBAMA | . 1 democrat | Thank you so much. Tonight, I am so thrilled a... | MICHELLE OBAMA | . 2 democrat | Thank you. It is a singular honor to be here t... | RICHARD DURBIN | . 3 democrat | Hey, Delaware. nAnd my favorite Democrat, Jil... | JOSEPH BIDEN | . 4 democrat | Hello. nThank you, Angie. I&#39;m so proud of how... | JILL BIDEN | . ... ... | ... | ... | . 184 republican | As the elected leader of 250,000 College Repub... | ALEX SCHRIVER | . 185 republican | Good afternoon. I&#39;m Pete Sessions, a congressm... | PETE SESSIONS | . 186 republican | To Chairman Priebus and to my fellow Americans... | BOB BUCKHORN | . 187 republican | nAbsolutely. Thank you, Mr.Chairman. nWelcome... | SHARON DAY | . 188 republican | I am thrilled to add Utah&#39;s voice in support f... | MIA LOVE | . 189 rows × 3 columns . Once you have the corpus ready, generate an interactive visualization in HTML: . html = st.produce_scattertext_explorer(corpus, category=&quot;democrat&quot;, category_name=&quot;Democratic&quot;, not_category_name=&quot;Republican&quot;, width_in_pixels=1000, metadata=convention_df[&quot;speaker&quot;]) . Now we&#39;ll render the HTML—give it a minute or two to load, it&#39;s worth the wait: . from IPython.display import IFrame file_name = &quot;foo.html&quot; with open(file_name, &quot;wb&quot;) as f: f.write(html.encode(&quot;utf-8&quot;)) IFrame(src=file_name, width = 1200, height=700) . Consider storing text from the last three years of customer support for a specific product in your company. Assume your team required information about how customers were discussing the product. This scattertext library has the potential to be extremely useful! You could cluster (k=2) on NPS (a customer evaluation metric) and then substitute the top two clustering components for the Democrat/Republican dimension. . Conclusion . In this blog, we introduce some frequently used features of spaCy from an overall perspective. If you asked many data scientists five years ago about open source in Python for natural language processing, the default answer would have been NLTK. That project includes almost everything except the kitchen sink and has mostly academic components. . There&#39;s so much more we can do with spaCy— hopefully, this tutorial provides an introduction. We wish you all the best in your natural language work. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/03/31/spaCy.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/03/31/spaCy.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 2",
            "content": "Required Packages . wordcloud, geopandas, nbformat, seaborn, scikit-learn . Now let’s get started! . First thing first, you need to load all the necessary libraries: . import pandas as pd from matplotlib import pyplot as plt import numpy as np from wordcloud import WordCloud from wordcloud import STOPWORDS import re import plotly.graph_objects as go import seaborn as sns . Data Cleaning . Following the previous blog, the raw data we scraped from Amazon look like below. . Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my github notobooks . Below is data after cleaning. It contains 6 columns and more than 500 rows. . . EDA . Below are the questions I curioused about, and the result generated by doing data analysis. . Which rating (1-5) got the most and least? . | Which country are they targeting? . | Which month people prefer to give a higher rating? . | Which month people leave commons the most? . | What are the useful words that people mentioned in the reviews? . | . . Sentiment Analysis . What is sentiment analysis? . Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as “somewhat positive” and “somewhat negative.” . In this specific case, we catogrize 4 and 5 stars to the positive group and 1 &amp; 2 stars to the negative gorup. . . Below are the most frequently words in reviews from positive group and negative group respectively. . Positive review . Negative review . Build up the first model . Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative. . Because this is a classification task, we will train a simple logistic regression model. . Clean Data First, we create a new function to remove all punctuations from the data for later use. | . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final . Split the Dataframe | . Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews. . . df[&#39;random_number&#39;] = np.random.randn(len(index)) train = df[df[&#39;random_number&#39;] &lt;= 0.8] test = df[df[&#39;random_number&#39;] &gt; 0.8] . Create a bag of words | . Here I would like to introduce a new package. . Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. . In this example, we are going to use sklearn.feature_extraction.text.CountVectorizer to convert a collection of text documents to a matrix of token counts. . The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text. . train_matrix = vectorizer.fit_transform(train[&#39;title&#39;]) test_matrix = vectorizer.transform(test[&#39;title&#39;]) . Import Logistic Regression from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . | Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] . | Fit model on data lr.fit(X_train,y_train) . | Make predictionsa predictions = lr.predict(X_test) . | . The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa. . Testing . Now, we can test the accuracy of our model! . from sklearn.metrics import confusion_matrix, classification_report new = np.asarray(y_test) confusion_matrix(predictions,y_test) print(classification_report(predictions,y_test)) . . The accuracy is as high as 89%! .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping2.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping2.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 1",
            "content": "Let’s get started! . . Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. . In this blog, we’ll look at scraping Amazon reviews with Python’s Scrapy. Scrapy is a python web crawling framework, and web scraping is a simple way of collecting data from many websites. Web scraping enables users to manage data for their specific needs, such as online merchandising, price monitoring, and marketing decision-making. . If you’re wondering whether or whether this procedure is legal, you can discover the answer here. . Before we get into scanning Amazon for product reviews, let’s take a look at some of the reasons why you might want to scrape Amazon reviews in the first place. . What exactly is Scrapy? . Scrapy is a web crawling framework that allows a developer to write code that defines how a specific site (or a group of websites) will be scraped. The most important feature is that it is built on Twisted, an asynchronous networking library, which significantly improves spider performance. . Why is it needed to scrape Amazon reviews? . Product reviews were subjected to a sentiment analysis. The reviews scraped from Amazon products can be subjected to sentiment analysis. This type of research aids in determining a user’s emotional response to a certain product. This can assist sellers, as well as other potential customers, in gaining a better understanding of public opinion about the goods. . | Dropshipping sales optimization Dropshipping is a business model that allows a company to operate without an inventory or a depository for product storage. You can utilize web scraping to acquire product pricing, user opinions, consumer wants, and trend information. . | Web scraping is used to keep track of a brand’s online reputation. It is tough for major corporations to keep track of their product reputation. Web scraping can aid in the extraction of pertinent review data, which can then be fed into various analysis tools to gauge user attitude toward the company. . | . The overall view . Examining the webpage’s HTML structure Scraping is the process of identifying patterns in web pages and extracting them. Before we begin writing a scraper, we must first understand the HTML structure of the target web page and identify patterns in it. The pattern is related to the repetitive use of classes, ids, and other HTML elements. . | Python scrapy parser implementation We work on the coded implementation in Python after analyzing the structure of the target web page. Scrapy parser’s responsibility is to visit the targeted web page and extract the information according to the rules specified. . | Information Gathering and Storage The parser can output the results in any format you want, including CSV and JSON. This is the final output where your scraped data is stored. . | . Shure MV7 . The product review we are going to scrap is the lastest dual USB;XLR microphone called Shure MV7. . . Shure is probabily the most well-known micphone company that Michael Jackson trusted. . . Examining the webpage’s HTML structure . At the very beginning, let’s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews . In actualality, the url can be shorten as below. . https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 . THis looks clearer now. We go the the Amazon’s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer “1” on the url represents the first page of the review pages. We can change it to 2, 3, … until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute “review-rating,” and review texts are represented by the class “review-text.” All we have to do now is use our Scrapy parser to pick up these patterns. . Python scrapy parser implementation . Install “scrapy” package . First, let’s install the scrapy package by enter the line below. . pip install scrapy . Then to create a scrapy project use following command. . scrapy startproject amazon_reviews_scraping . After entering the code, you should see these two. One is a folder that contains your scrapy code, and the other is your spacy configuration file. Spacy configuration while helps in running and deploying the Scrapy project on a server. . . We’ll need to creat a spider once we have a project in place. A spider is a piece of Python code that controls how a web page is scraped. It is the main component that crawls various web pages and extracts content from them. In our case, this will be the code chuck that will visit Amazon and scrape Amazon reviews. You can use the following command to make a spider. . scrapy genspider amazon_review your-link-here . The structure is like the image below. . . Files description . items.py Items are containers that will be loaded with the scraped data. . | Middleware.py The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders. . | Pipelines.py After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class. . | settings.py It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. . | spiders folder The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. . | . Here is the deeper explaination about each files. . Defining Scrapy Parser in Python . Now we go to the spider folder and open the file reviews.py. In this process, we create two variables which are reviews_url and asin_list for convience later. Then we create a new function to request the website. . . Then we download another package called scraper-helper to make the process even easier. . pip install scraper-helper . Now we go to folder Amazon_reviews_scraping and open settings.py. We delete everything but these lines. . . We use the new import library method “get_dict” to convert the string lines from comments to dictions format. You can simply copy and paste this piece because this process required some html knowledge. After this, we fit the request header and can successfully connect to the website. . The next step is to go back the review.py and finish the function “parse”. We open the inspect tool and can easily find that each star rating, data to review, and comments about the preduct has their own type unique headers, such as “[data-hook=”review-star-rating”]” for star rating. We find each unique header and get each type of information we need. . . Of course, after examining the css file, we can create a few lines to make sure after we scrape the date, it jump to the next page automatically and make sure we can collect enought data we need. . . Eventually, we run the code below by using terminal. . scrapy crawl reviews -o new_file_name.csv . This will generate a new csv file to help us to store the data we request from Amazon. . Conclusion . We did it! . Even though the whole process looks easy, but there are still some unexpected things that are warting for you to explore. For example, for star rating, different countries have different headers like below. . United States: . [data-hook=”review-star-rating”] . Other countries: . [data-hook=”cmps-review-star-rating”] . Another difficulty in scraping Amazon reviews is that Amazon tends to block IPs if you scrape Amazon frequently. This can be an impediment to your work. . In such cases, make sure to rotate your IP addresses on a regular basis and make fewer requests to the Amazon server to avoid being blocked. More information can be found here. . In the next blog, part 2, we are going to clean up the data and dig in the sentiment analysis. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping1.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping1.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Predicting Stock Change With Python",
            "content": "Stock is a highly sensitive and turbulent market. Because of the recent crisis between Russia and Ukraine, for example, a few comments from Putin or another powerful figure might lead millions of people to lose or make profit in a matter of minutes. A essential skill that modern people should have is the ability to foresee trends in order to preserve their investments and maximize their profits. In this blog, we are going to introduce three basic functions to support you to achieve the following goals. . First, selenium is the first option to support us to do web scraping from Yahoo finance based on the filter we set up, such as Aggressive Small Caps. Here is a Youtube selenium tutoria that I recommend you to set up selenium. And use this selenium python tutorial as more detailed reference. Then, we will get the historical data of that most active stock. Second, we will predict the stock trends. Eventually, we send out the predictions and the lastest change on the stock to our receiver by email. . . Set up the envirnment . Bleow are some packages that are necessary to run the code. . import pandas as pd import numpy as np import re from getpass import getpass from datetime import datetime, date, time, timezone import smtplib from selenium import webdriver import os #For Prediction from sklearn.linear_model import LinearRegression from sklearn import preprocessing from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from selenium.webdriver.common.by import By from selenium.webdriver.chrome.service import Service #For Stock Data from iexfinance.stocks import get_historical_data from iexfinance.stocks import Stock . Do not have enough stock data? . Method one: scrapying the stock data from Yahoo for free! . After following the tutorial at the very beginning of this blog, we can create our chrome drive and use driver.get(url) navigate to our desired webpage: Yahoo finance which displays the top 25 most active stocks by default. You can also change the filter based on what you are looking for. Inside webdriver.Chrome() you will need to type your chromedriver path. . After connecting to Yahoo by using webdriver, we could use a double for loops to crawl the whole database on the website. In detail, &quot;j&quot; represents how many rows we need, and &quot;i&quot; represents which column we need. . def getStocks(n): #Navigating to the Yahoo stock screener driver = webdriver.Chrome(service= Service( &#39;/Users/zeyu/Desktop/chromedriver&#39;)) url = &quot;https://finance.yahoo.com/screener/predefined/aggressive_small_caps?offset=0&amp;count=202&quot; driver.get(url) #Creating a stock list and iterating through the ticker names on the stock screener list data = [[] for i in range(9)] for i in range(1,len(data)+1): for j in range(1,n+1): ticker = driver.find_element(By.XPATH, &#39;//*[@id = &quot;scr-res-table&quot;]/div[1]/table/tbody/tr[&#39; + str(j) + &#39;]/td[&#39; + str(i) + &#39;]&#39;) data[i-1].append(ticker.text) driver.quit() #Using the stock list to predict the future price of the stock a specificed amount of days number = 0 for i in data: print(&quot;Number: &quot; + str(number)) try: predictData(i, 5) except: print(&quot;Stock: &quot; + i + &quot; was not predicted&quot;) number += 1 if __name__ == &#39;__main__&#39;: getStocks(20) . Method two: applying historical stock data from IEX Cloud with cost. . Before everything, you need to visit iexcloud to create an account and get a exclusive API. The free version only offers a very limited access. . Now, we can using the get_historical_data() package to get cleaned up dataset you want. Then there are a few parameters you need to enter. First, you need to enter the stock symblo like (AAPL). Then, set the start and end date, the output format (we will use pandas in this project), and eventually, the token which is API you acquired from the iexcloud website. . start = datetime(2021, 2, 17) end = datetime(2022, 2, 16) API = getpass(&quot;Please enter your API&quot;) df = get_historical_data(stock_symblo, start=start, end=end, output_format=&#39;pandas&#39;, token = API) . df = pd.read_csv(&#39;/Users/zeyu/Desktop/DS/Stock/Selenium/SPOT_df.csv&#39;) df = df.drop([&#39;subkey&#39;], axis = 1) pd.set_option(&#39;display.max_columns&#39;, None) df = df.rename(columns = {&quot;Unnamed: 0&quot; : &quot;date&quot;}) df.head() . container = [] for i in range(len(df.date)): x = re.sub((&quot;-&quot;), &quot;&quot;, df.date[i]) container.append(x) df.date = container df = df.drop([&#39;symbol&#39;, &quot;id&quot;, &quot;key&quot;, &quot;label&quot;], axis = 1) . Predict the future stock! . In the following lines, we first create a prediction column called &quot;Prediction&quot;, each day&#39;s close price is the prediction of the previous day. Second, two datasets are created. X is the predictor variable, Y is the target variable. Preprocessing package is to nomalize our predictor variables. Then we split our data into train and test datasets for both X and Y. Then we use the Regression on the training data, then predict the closing price of X_prediction. . def predictData(stock, days): if os.path.exists(&#39;./Exports&#39;): csv_name = (&#39;Exports/&#39; + stock + &#39;_Export.csv&#39;) else: os.mkdir(&quot;Exports&quot;) csv_name = (&#39;Exports/&#39; + stock + &#39;_Export.csv&#39;) df.to_csv(csv_name) df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) df.dropna(inplace = True) forecast_time = int(days) #Predicting the Stock price in the future X = np.array(df.drop([&#39;prediction&#39;], axis=1)) Y = np.array(df[&#39;prediction&#39;]) #Nomalize our predictor variables X = preprocessing.scale(X) print(&quot; nX aftre preprocessing:&quot;, X) X_prediction = X[-forecast_time:] # Split our data into train and test data X_train, X_test, Y_train, Y_Test = train_test_split(X, Y, test_size=0.5) print(&quot; n nX train:&quot;, X_train, &quot; nX_test:&quot;,X_test, &quot; nY_train:&quot;, Y_train, &quot; nY_Test&quot;, Y_Test) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) #Predict the closing price of X_prediction. prediction = (clf.predict(X_prediction)) last_row = df.tail(1) print(&quot;Last row:&quot;, last_row) print(&quot;Last row 1:&quot;, last_row[&#39;close&#39;]) #Sending the SMS if the predicted price of the stock is at least 1 greater than the previous closing price if (float(prediction[4]) &gt; (float(last_row[&#39;close&#39;])) + 1): output = (&quot; n nStock:&quot; + str(stock) + &quot; nPrior Close: n&quot; + str(last_row[&#39;close&#39;]) + &quot; n nPrediction in 1 Day: &quot; + str( prediction[0]) + &quot; nPrediction in 5 Days: &quot; + str(prediction[4])) #sendMessage(output) print(&quot;This is the output:&quot;, output) predictData(df, 5) . Magic that send message through gmail! . Eventually, the last function sendMessage(text) is for us to send messages through gamil when some conditions are triggered by using smtplib module. getpass() is another package use here for user enter the password to improve the security. . def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = getpass(&quot;Please enter your gmail address like chrisguan912@gmail.com&quot;) password = getpass(&quot;Please enter your password&quot;) vtext = getpass(&quot;Please enter the receiver&#39;s phone number and company&#39;s domain like 9292649156@txt.att.net&quot;) message = getpass(&quot;Please enter your message&quot;) MSG = &quot;&quot;&quot;From: %s n To: %s n %s &quot;&quot;&quot; %(username, vtext, message) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, vtext, MSG) server.quit() print(&#39;Sent&#39;) . Conclusion . The blog introduced four functions in total. The first two can support us to apply valuable data by using selenium or download from iexcloud. The third funciton is to analyze the predict the stock trend. The last few lines support us to send a message through gmail. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/selenium/python/iex/yahoo/2022/02/24/Predict.html",
            "relUrl": "/selenium/python/iex/yahoo/2022/02/24/Predict.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
  
    
        ,"post8": {
            "title": "Resume",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/02/09/Resume.html",
            "relUrl": "/2020/02/09/Resume.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Abou Me",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/01/28/Abou-Me.html",
            "relUrl": "/2020/01/28/Abou-Me.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . I may be a hypocrite, but I’m not wrong… . Example Markdown Post . . I’ve spent over 25 years preaching the value of a portfolio site to artists, software engineers, and designers that wanted to join my studio or learn how to get into games or related industries. Now that I’m 6 months into my PhD and planning for career 2.0, it’s past time for me to do the same. This sums up the justification in a single tweet: . The popularity of this tweet makes me laugh, because it&#39;s an illustrative example! I thought about trying to get a screenshot of David&#39;s slide, but just took a terrible phone photo of a webinar and tweeted with the quote. . &mdash; Amelia McNamara (@AmeliaMN) July 22, 2018 I recently finished reading Build a Career in Data Science2, which I recommend. It motivated me to start the process of sharing my work. . Introduction to Fastpages . This site is built with fastpages, a platform for building static Jekyll web blogs from Jupyter Notebooks (JNs). Markdown and Word DOCs are also supported. . . From their github page: . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. . Fastpages is built by the team that created the deep learning PyTorch front-end fastai and the Python programming environment nbdev. The team is led by Jeremy Howard, former President and Chief Data Scientist of Kaggle and author of Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD1. . I was first introduced fastpages and nbdev during the recent ACM TechTalk, It’s Time Deep Learning Learned from Software Engineering, presented by Howard and moderated by his collaborator Hamel Husain. . From Introducing Fastpages: . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . Some resources I found helpful in launching this blog: . The fastpages github page has detailed setup instructions | This video tutorial created by Abdul Majed walks you through the initial setup process | A sample jupyter-based blog page provides a live demonstration of its capabilities | . I was able to get the initial site live quite quickly with little drama, and have found the process of modifying existing content and publishing new material straightforward so far. Before choosing fastpages I considered the following hosting / authoring options: . A general purpose site like Wix | A page on Medium or Substack | Other JN-based tools like Pelican | Finding a way to use JNs in blogdown3 | . I decided that Fastpages was the best way for me to publish JN-based work directly, with professional look and feel, on a site that I control, for free. That’s a pretty great combination. I also like that it supports Markdown (like this page), is part of the larger fast.ai family of products (and philosophy), and is hosted on github. . Finally, if you still need to be convinced of the value of building a portfolio, and what to include in one give the following resources a look: . Advice to aspiring data scientists: start a blog on David Robinson’s Variance Explained | Data Science Portfolios That Will Get You the Job | Thinking of blogging about Data Science? Here are some tips and possible benefits. | . Many of us need to spend less time consuming, and more time creating. Now you know how and why, so go publish something! . . Howard, Jeremy, and Sylvain Gugger. Deep Learning for Coders with Fastai and PyTorch. O’Reilly Media, Inc., 2020. &#8617; &#8617;2 . | Robinson, Emily, and Jacqueline Nolis. Build a Career in Data Science. Simon and Schuster, 2020. &#8617; . | There is a lot of cool stuff going on in R these days and blogdown supports a lot of interesting publishing workflows, but I ultimately decided that it would probably be easier to find a way to publish my R work using fastpages than to publish my Python work with blogdown. Time will tell… &#8617; . |",
            "url": "https://christopherguan.github.io/sample-ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Chris",
          "content": ". Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . Experience . Youtube Data Automation                                                              December 2021 – January 2022 Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. | Cleaned and stored data to MySQL database by using python. | Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. | . . CRM System                                                                                         May 2021– September 2021 Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. | Implemented and applied requirements from financial department to four popular applications. | Created multiple video tutorials to help the future employees to understand the application better. | . . Data Mining                                                                                               January 2021 – May 2021 Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. | Master on Pandas and Numpy python packages. | Developed movie recommendation system based on both content-based and collaborative filtering. | . . Business Data Analytics                                                                           May 2020 – August 2020 NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. | . . Database System                                                                                      January 2020 – May 2020 Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. | Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. | . . Statistical Computing and Graphics                                        September 2020 – December 2020 World Happiness Analysis . Performed data manipulation and data cleaning. | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. | Analyzed and predicted the causation. | . . Skills . Computer Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. Analytical skills, qualitative and quantitative research skills. Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . . Certifications . Data Analysis in Spreadsheets | Intermediate R for Finance | Credit Risk Modeling in R | . . Favorite Quote . “Great beauty, great strength, and great riches are really and truly of no great use; a right heart exceeds all” ― Benjamin Franklin .",
          "url": "https://christopherguan.github.io/sample-ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://christopherguan.github.io/sample-ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}