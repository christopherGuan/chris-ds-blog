{
  
    
        "post0": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 2",
            "content": "Required Packages . wordcloud, geopandas, nbformat, seaborn, scikit-learn . Now let’s get started! . First thing first, you need to load all the necessary libraries: . import pandas as pd from matplotlib import pyplot as plt import numpy as np from wordcloud import WordCloud from wordcloud import STOPWORDS import re import plotly.graph_objects as go import seaborn as sns . Data Cleaning . Following the previous blog, the raw data we scraped from Amazon look like below. . Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my github notobooks . Below is data after cleaning. It contains 6 columns and more than 500 rows. . . EDA . Below are the questions I curioused about, and the result generated by doing data analysis. . Which rating (1-5) got the most and least? . | Which country are they targeting? . | Which month people prefer to give a higher rating? . | Which month people leave commons the most? . | What are the useful words that people mentioned in the reviews? . | . . Sentiment Analysis . What is sentiment analysis? . Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as “somewhat positive” and “somewhat negative.” . In this specific case, we catogrize 4 and 5 stars to the positive group and 1 &amp; 2 stars to the negative gorup. . . Below are the most frequently words in reviews from positive group and negative group respectively. . Positive review . Negative review . Build up the first model . Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative. . Because this is a classification task, we will train a simple logistic regression model. . Clean Data First, we create a new function to remove all punctuations from the data for later use. | . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final . Split the Dataframe | . Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews. . . df[&#39;random_number&#39;] = np.random.randn(len(index)) train = df[df[&#39;random_number&#39;] &lt;= 0.8] test = df[df[&#39;random_number&#39;] &gt; 0.8] . Create a bag of words | . Here I would like to introduce a new package. . Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. . In this example, we are going to use sklearn.feature_extraction.text.CountVectorizer to convert a collection of text documents to a matrix of token counts. . The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text. . train_matrix = vectorizer.fit_transform(train[&#39;title&#39;]) test_matrix = vectorizer.transform(test[&#39;title&#39;]) . Import Logistic Regression from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . | Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] . | Fit model on data lr.fit(X_train,y_train) . | Make predictionsa predictions = lr.predict(X_test) . | . The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa. . Testing . Now, we can test the accuracy of our model! . from sklearn.metrics import confusion_matrix, classification_report new = np.asarray(y_test) confusion_matrix(predictions,y_test) print(classification_report(predictions,y_test)) . . The accuracy is as high as 89%! .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping2.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping2.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping Amazon Reviews using Scrapy in Python",
            "content": "Scraping Amazon Reviews using Scrapy in Python . . Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. . In this blog, we’ll look at scraping Amazon reviews with Python’s Scrapy. Scrapy is a python web crawling framework, and web scraping is a simple way of collecting data from many websites. Web scraping enables users to manage data for their specific needs, such as online merchandising, price monitoring, and marketing decision-making. . If you’re wondering whether or whether this procedure is legal, you can discover the answer here. . Before we get into scanning Amazon for product reviews, let’s take a look at some of the reasons why you might want to scrape Amazon reviews in the first place. . What exactly is Scrapy? . Scrapy is a web crawling framework that allows a developer to write code that defines how a specific site (or a group of websites) will be scraped. The most important feature is that it is built on Twisted, an asynchronous networking library, which significantly improves spider performance. . Why is it needed to scrape Amazon reviews? . Product reviews were subjected to a sentiment analysis. The reviews scraped from Amazon products can be subjected to sentiment analysis. This type of research aids in determining a user’s emotional response to a certain product. This can assist sellers, as well as other potential customers, in gaining a better understanding of public opinion about the goods. . | Dropshipping sales optimization Dropshipping is a business model that allows a company to operate without an inventory or a depository for product storage. You can utilize web scraping to acquire product pricing, user opinions, consumer wants, and trend information. . | Web scraping is used to keep track of a brand’s online reputation. It is tough for major corporations to keep track of their product reputation. Web scraping can aid in the extraction of pertinent review data, which can then be fed into various analysis tools to gauge user attitude toward the company. . | . The overall view . Examining the webpage’s HTML structure Scraping is the process of identifying patterns in web pages and extracting them. Before we begin writing a scraper, we must first understand the HTML structure of the target web page and identify patterns in it. The pattern is related to the repetitive use of classes, ids, and other HTML elements. . | Python scrapy parser implementation We work on the coded implementation in Python after analyzing the structure of the target web page. Scrapy parser’s responsibility is to visit the targeted web page and extract the information according to the rules specified. . | Information Gathering and Storage The parser can output the results in any format you want, including CSV and JSON. This is the final output where your scraped data is stored. . | . Shure MV7 . The product review we are going to scrap is the lastest dual USB;XLR microphone called Shure MV7. . . Shure is probabily the most well-known micphone company that Michael Jackson trusted. . . Examining the webpage’s HTML structure . At the very beginning, let’s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews . In actualality, the url can be shorten as below. . https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 . THis looks clearer now. We go the the Amazon’s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer “1” on the url represents the first page of the review pages. We can change it to 2, 3, … until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute “review-rating,” and review texts are represented by the class “review-text.” All we have to do now is use our Scrapy parser to pick up these patterns. . Python scrapy parser implementation . Install “scrapy” package . First, let’s install the scrapy package by enter the line below. . pip install scrapy . Then to create a scrapy project use following command. . scrapy startproject amazon_reviews_scraping . After entering the code, you should see these two. One is a folder that contains your scrapy code, and the other is your spacy configuration file. Spacy configuration while helps in running and deploying the Scrapy project on a server. . . We’ll need to creat a spider once we have a project in place. A spider is a piece of Python code that controls how a web page is scraped. It is the main component that crawls various web pages and extracts content from them. In our case, this will be the code chuck that will visit Amazon and scrape Amazon reviews. You can use the following command to make a spider. . scrapy genspider amazon_review your-link-here . The structure is like the image below. . . Files description . items.py Items are containers that will be loaded with the scraped data. . | Middleware.py The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders. . | Pipelines.py After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class. . | settings.py It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. . | spiders folder The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. . | . Here is the deeper explaination about each files. . Defining Scrapy Parser in Python . Now we go to the spider folder and open the file reviews.py. In this process, we create two variables which are reviews_url and asin_list for convience later. Then we create a new function to request the website. . . Then we download another package called scraper-helper to make the process even easier. . pip install scraper-helper . Now we go to folder Amazon_reviews_scraping and open settings.py. We delete everything but these lines. . . We use the new import library method “get_dict” to convert the string lines from comments to dictions format. You can simply copy and paste this piece because this process required some html knowledge. After this, we fit the request header and can successfully connect to the website. . The next step is to go back the review.py and finish the function “parse”. We open the inspect tool and can easily find that each star rating, data to review, and comments about the preduct has their own type unique headers, such as “[data-hook=”review-star-rating”]” for star rating. We find each unique header and get each type of information we need. . . Of course, after examining the css file, we can create a few lines to make sure after we scrape the date, it jump to the next page automatically and make sure we can collect enought data we need. . . Eventually, we run the code below by using terminal. . scrapy crawl reviews -o new_file_name.csv . This will generate a new csv file to help us to store the data we request from Amazon. . Conclusion . We did it! . Even though the whole process looks easy, but there are still some unexpected things that are warting for you to explore. For example, for star rating, different countries have different headers like below. . United States: . [data-hook=”review-star-rating”] . Other countries: . [data-hook=”cmps-review-star-rating”] . Another difficulty in scraping Amazon reviews is that Amazon tends to block IPs if you scrape Amazon frequently. This can be an impediment to your work. . In such cases, make sure to rotate your IP addresses on a regular basis and make fewer requests to the Amazon server to avoid being blocked. More information can be found here. . In the next blog, part 2, we are going to clean up the data and dig in the sentiment analysis. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping1.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping1.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "Resume",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/02/09/Resume.html",
            "relUrl": "/2020/02/09/Resume.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Abou Me",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/01/28/Abou-Me.html",
            "relUrl": "/2020/01/28/Abou-Me.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://christopherguan.github.io/sample-ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Chris",
          "content": ". Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . Experience . Youtube Data Automation                                                              December 2021 – January 2022 Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. | Cleaned and stored data to MySQL database by using python. | Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. | . . CRM System                                                                                         May 2021– September 2021 Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. | Implemented and applied requirements from financial department to four popular applications. | Created multiple video tutorials to help the future employees to understand the application better. | . . Data Mining                                                                                               January 2021 – May 2021 Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. | Master on Pandas and Numpy python packages. | Developed movie recommendation system based on both content-based and collaborative filtering. | . . Business Data Analytics                                                                           May 2020 – August 2020 NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. | . . Database System                                                                                      January 2020 – May 2020 Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. | Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. | . . Statistical Computing and Graphics                                        September 2020 – December 2020 World Happiness Analysis . Performed data manipulation and data cleaning. | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. | Analyzed and predicted the causation. | . . Skills . Computer Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. Analytical skills, qualitative and quantitative research skills. Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . . Certifications . Data Analysis in Spreadsheets | Intermediate R for Finance | Credit Risk Modeling in R | . . Favorite Quote . “Great beauty, great strength, and great riches are really and truly of no great use; a right heart exceeds all” ― Benjamin Franklin .",
          "url": "https://christopherguan.github.io/sample-ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://christopherguan.github.io/sample-ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}