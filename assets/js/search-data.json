{
  
    
        "post0": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 2",
            "content": "Required Packages . wordcloud, geopandas, nbformat, seaborn, scikit-learn . Now let&#39;s get started! . First thing first, you need to load all the necessary libraries: . import pandas as pd from matplotlib import pyplot as plt import numpy as np from wordcloud import WordCloud from wordcloud import STOPWORDS import re import plotly.graph_objects as go import seaborn as sns . Data Cleaning . Following the previous blog, the raw data we scraped from Amazon look like below. . Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my github notebooks. . Below is data after cleaning. It contains 6 columns and more than 500 rows. . . EDA . Below are the questions I curioused about, and the result generated by doing data analysis. . Which rating (1-5) got the most and least? . | Which country are they targeting? . | Which month people prefer to give a higher rating? . | Which month people leave commons the most? . | . What are the useful words that people mentioned in the reviews? | . . Sentiment Analysis (Method 1) . What is sentiment analysis? . Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as &quot;somewhat positive&quot; and &quot;somewhat negative.&quot; . In this specific case, we catogrize 4 and 5 stars to the positive group and 1 &amp; 2 stars to the negative gorup. . . Below are the most frequently words in reviews from positive group and negative group respectively. . Positive review . Negative review . Build up the first model . Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative. . Because this is a classification task, we will train a simple logistic regression model. . Clean Data First, we create a new function to remove all punctuations from the data for later use. | . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final . Split the Dataframe | . Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews. . . df[&#39;random_number&#39;] = np.random.randn(len(index)) train = df[df[&#39;random_number&#39;] &lt;= 0.8] test = df[df[&#39;random_number&#39;] &gt; 0.8] . Create a bag of words | . Here I would like to introduce a new package. . Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. . In this example, we are going to use sklearn.feature_extraction.text.CountVectorizer to convert a collection of text documents to a matrix of token counts. . The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text. . train_matrix = vectorizer.fit_transform(train[&#39;title&#39;]) test_matrix = vectorizer.transform(test[&#39;title&#39;]) . Import Logistic Regression | . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . Split target and independent variables | . X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] . Fit model on data | . lr.fit(X_train,y_train) . Make predictionsa | . predictions = lr.predict(X_test) . The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa. . Testing . Now, we can test the accuracy of our model! . from sklearn.metrics import confusion_matrix, classification_report new = np.asarray(y_test) confusion_matrix(predictions,y_test) print(classification_report(predictions,y_test)) . . The accuracy is as high as 89%! . Sentiment Analysis (Method 2) . In this process, you will learn how to build your own sentiment analysis classifier using Python and understand the basics of NLP (natural language processing). First, let&#39;s try to use a quick and dirty method to utilize the Naive Bayes classifier to predict the sentiments of Amazon product review. . Based on the application&#39;s requirements, we should first put each review in a txt file and catogorize them as negative or positive review in different folder. . neg = df[df[&quot;sentiment&quot;] == -1].review #Reset the index neg.index = range(len(neg.index)) ## Write each DataFrame to separate txt for i in range(len(neg)): data = neg[i] with open(str(i) + &quot;.txt&quot;,&quot;w&quot;) as file: file.write(data + &quot; n&quot;) . Next, we sort the order of the official data and remove all the content. In other words, we only keep the file name. . import os import pandas as pd #Get file names file_names = os.listdir(&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg&#39;) #Convert pandas neg_df = pd.DataFrame (file_names, columns = [&#39;file_name&#39;]) #split to sort neg_df[[&#39;number&#39;,&#39;id&#39;]] = neg_df.file_name.apply( lambda x: pd.Series(str(x).split(&quot;_&quot;))) #change the number to be the index neg_df_index = neg_df.set_index(&#39;number&#39;) neg_org = neg_df_index.sort_index(ascending=True) #del neg[&quot;id&quot;] neg_org.reset_index(inplace=True) neg_org = neg_org.drop([0], axis=0).reset_index(drop=True) neg_names = neg_org[&#39;file_name&#39;] for file_name in neg_names: t = open(f&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{file_name}&#39;, &#39;w&#39;) t.write(&quot;&quot;) t.close() . Next, we insert the content of amazon review to the official files with their original file names. . file_names = os.listdir(&#39;/Users/zeyu/Desktop/DS/neg&#39;) #Convert pandas pos_df = pd.DataFrame (file_names, columns = [&#39;file_name&#39;]) pos_names = pos_df[&#39;file_name&#39;] for index, file_name in enumerate(pos_names): try: t = open(f&#39;/Users/zeyu/Desktop/DS/neg/{file_name}&#39;, &#39;r&#39;) # t.write(&quot;&quot;) t_val = ascii(t.read()) t.close() writefname = pos_names_org[index] t = open(f&#39;/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{writefname}&#39;, &#39;w&#39;) t.write(t_val) t.close() except: print(f&#39;{index} Reading/writing Error&#39;) . Eventually, we can just run these few lines to predict the sentiments of Amazon product review. . import nltk from nltk.corpus import movie_reviews import random documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)] #All words, not unique. random.shuffle(documents) . all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) #Only show first 2000. word_features = list(all_words)[:2000] def document_features(document): document_words = set(document) features = {} for word in word_features: features[&#39;contains({})&#39;.format(word)] = (word in document_words) return features . featuresets = [(document_features(d), c) for (d,c) in documents] train_set, test_set = featuresets[100:], featuresets[:100] classifier = nltk.NaiveBayesClassifier.train(train_set) print(nltk.classify.accuracy(classifier, test_set)) . classifier.show_most_informative_features(5) .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping2.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping2.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 1",
            "content": "Let&#39;s get started! . Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. . In this blog, we&#39;ll look at scraping Amazon reviews with Python&#39;s Scrapy. Scrapy is a python web crawling framework, and web scraping is a simple way of collecting data from many websites. Web scraping enables users to manage data for their specific needs, such as online merchandising, price monitoring, and marketing decision-making. . If you&#39;re wondering whether or whether this procedure is legal, you can discover the answer here. . Before we get into scanning Amazon for product reviews, let&#39;s take a look at some of the reasons why you might want to scrape Amazon reviews in the first place. . What exactly is Scrapy? . Scrapy is a web crawling framework that allows a developer to write code that defines how a specific site (or a group of websites) will be scraped. The most important feature is that it is built on Twisted, an asynchronous networking library, which significantly improves spider performance. . Why is it needed to scrape Amazon reviews? . Product reviews were subjected to a sentiment analysis. The reviews scraped from Amazon products can be subjected to sentiment analysis. This type of research aids in determining a user&#39;s emotional response to a certain product. This can assist sellers, as well as other potential customers, in gaining a better understanding of public opinion about the goods. . | Dropshipping sales optimization Dropshipping is a business model that allows a company to operate without an inventory or a depository for product storage. You can utilize web scraping to acquire product pricing, user opinions, consumer wants, and trend information. . | Web scraping is used to keep track of a brand&#39;s online reputation. It is tough for major corporations to keep track of their product reputation. Web scraping can aid in the extraction of pertinent review data, which can then be fed into various analysis tools to gauge user attitude toward the company. . | . The overall view . Examining the webpage&#39;s HTML structure Scraping is the process of identifying patterns in web pages and extracting them. Before we begin writing a scraper, we must first understand the HTML structure of the target web page and identify patterns in it. The pattern is related to the repetitive use of classes, ids, and other HTML elements. . | Python scrapy parser implementation We work on the coded implementation in Python after analyzing the structure of the target web page. Scrapy parser&#39;s responsibility is to visit the targeted web page and extract the information according to the rules specified. . | Information Gathering and Storage The parser can output the results in any format you want, including CSV and JSON. This is the final output where your scraped data is stored. . | . Shure MV7 . The product review we are going to scrap is the lastest dual USB;XLR microphone called Shure MV7. . . Shure is probabily the most well-known micphone company that Michael Jackson trusted. . . Examining the webpage&#39;s HTML structure . At the very beginning, let&#39;s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews In actualality, the url can be shorten as below. https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 THis looks clearer now. We go the the Amazon&#39;s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer &quot;1&quot; on the url represents the first page of the review pages. We can change it to 2, 3, ... until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute &quot;review-rating,&quot; and review texts are represented by the class &quot;review-text.&quot; All we have to do now is use our Scrapy parser to pick up these patterns. . Examining the webpage&#39;s HTML structure . At the very beginning, let&#39;s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews In actualality, the url can be shorten as below. https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 THis looks clearer now. We go the the Amazon&#39;s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer &quot;1&quot; on the url represents the first page of the review pages. We can change it to 2, 3, ... until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute &quot;review-rating,&quot; and review texts are represented by the class &quot;review-text.&quot; All we have to do now is use our Scrapy parser to pick up these patterns. . Python scrapy parser implementation . Install &quot;scrapy&quot; package . First, let&#39;s install the scrapy package by enter the line below. . pip install scrapy . Then to create a scrapy project use following command. . scrapy startproject amazon_reviews_scraping . After entering the code, you should see these two. One is a folder that contains your scrapy code, and the other is your spacy configuration file. Spacy configuration while helps in running and deploying the Scrapy project on a server. . We&#39;ll need to creat a spider once we have a project in place. A spider is a piece of Python code that controls how a web page is scraped. It is the main component that crawls various web pages and extracts content from them. In our case, this will be the code chuck that will visit Amazon and scrape Amazon reviews. You can use the following command to make a spider. . scrapy genspider amazon_review your-link-here . The structure is like the image below. . . Files description . items.py Items are containers that will be loaded with the scraped data. | . Middleware.py The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders. | . Pipelines.py After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class. | . settings.py It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. | . spiders folder The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. | . Here is the deeper explaination about each files. . Defining Scrapy Parser in Python . Now we go to the spider folder and open the file reviews.py. In this process, we create two variables which are reviews_url and asin_list for convience later. Then we create a new function to request the website. . reviews_url = &#39;https://www.amazon.com/product-reviews/{}&#39; asin_list = [&#39;B08G7RG9ML&#39;] class ReviewsSpider(scrapy.Spider): name = &#39;reviews&#39; def start_requests(self): for asin in asin_list: url = reviews_url.format(asin) yield scrapy.Request(url) . Then we download another package called scraper-helper to make the process even easier. . pip install scraper-helper . Now we go to folder Amazon_reviews_scraping and open settings.py. We delete everything but these lines. . import scraper_helper as sh BOT_NAME = &#39;Amazon_reviews_scraping&#39; SPIDER_MODULES = [&#39;Amazon_reviews_scraping.spiders&#39;] NEWSPIDER_MODULE = &#39;Amazon_reviews_scraping.spiders&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False DEFAULT_REQUEST_HEADERS = sh.get_dict( &quot;&quot;&quot; accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9, accept-encoding: deflate, accept-language: en-US,en;q=0.9, sec-ch-ua: &quot;Chromium&quot;;v=&quot;88&quot;, &quot;Google Chrome&quot;;v=&quot;88&quot;, &quot;;Not A Brand&quot;;v=&quot;99&quot;, sec-ch-ua-mobile: ?0, sec-fetch-dest: document, sec-fetch-mode: navigate, sec-fetch-site: none, sec-fetch-user: ?1, upgrade-insecure-requests: 1, user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36 &quot;&quot;&quot; ) . We use the new import library method &quot;get_dict&quot; to convert the string lines from comments to dictions format. You can simply copy and paste this piece because this process required some html knowledge. After this, we fit the request header and can successfully connect to the website. . The next step is to go back the review.py and finish the function &quot;parse&quot;. We open the inspect tool and can easily find that each star rating, data to review, and comments about the preduct has their own type unique headers, such as &quot;[data-hook=&quot;review-star-rating&quot;]&quot; for star rating. We find each unique header and get each type of information we need. . import scrapy reviews_url = &#39;https://www.amazon.com/product-reviews/{}&#39; asin_list = [&#39;B08G7RG9ML&#39;] class ReviewsSpider(scrapy.Spider): name = &#39;reviews&#39; def start_requests(self): for asin in asin_list: url = reviews_url.format(asin) yield scrapy.Request(url) def parse(self, response): #print(&#39;Im in parse&#39;) for review in response.css(&#39;[data-hook=&quot;review&quot;]&#39;): item = { &#39;name&#39;: review.css(&#39;.a-profile-name::text&#39;).get(), &#39;stars1&#39;: review.css (&#39;[data-hook=&quot;review-star-rating&quot;] ::text&#39;) .get (), &#39;stars2&#39;: review.css (&#39;[data-hook=&quot;cmps-review-star-rating&quot;] ::text&#39;) .get (), &#39;date&#39;: review.css (&#39;[data-hook=&quot;review-date&quot;] ::text&#39;) .get (), &#39;title&#39;: review.css (&#39;[data-hook=&quot;review-title&quot;] span ::text&#39;) .get (), &#39;review&#39;: review.xpath(&#39;normalize-space(.//*[@data-hook=&quot;review-body&quot;])&#39;).get() } yield item next_page = response.xpath(&#39;//a[text ()=&quot;Next page&quot;]/@href&#39;).get() if next_page: yield scrapy.Request(response.urljoin(next_page)) . Of course, after examining the css file, we can create a few lines to make sure after we scrape the date, it jump to the next page automatically and make sure we can collect enought data we need. . next_page = response.xpath(&#39;//a[text ()=&quot;Next page&quot;]/@href&#39;).get() if next_page: yield scrapy.Request(response.urljoin(next_page)) . Eventually, we run the code below by using terminal. . scrapy crawl reviews -o new_file_name.csv . This will generate a new csv file to help us to store the data we request from Amazon. . Conclusion . We did it! . Even though the whole process looks easy, but there are still some unexpected things that are warting for you to explore. For example, for star rating, different countries have different headers like below. . United States: . [data-hook=&quot;review-star-rating&quot;] . Other countries:&gt; [data-hook=&quot;cmps-review-star-rating&quot;] . Another difficulty in scraping Amazon reviews is that Amazon tends to block IPs if you scrape Amazon frequently. This can be an impediment to your work. . In such cases, make sure to rotate your IP addresses on a regular basis and make fewer requests to the Amazon server to avoid being blocked. More information can be found here. . In the next blog, part 2, we are going to clean up the data and dig in the sentiment analysis. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping1.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2022/04/07/scraping1.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 2",
            "content": "Required Packages . wordcloud, geopandas, nbformat, seaborn, scikit-learn . Now let’s get started! . First thing first, you need to load all the necessary libraries: . import pandas as pd from matplotlib import pyplot as plt import numpy as np from wordcloud import WordCloud from wordcloud import STOPWORDS import re import plotly.graph_objects as go import seaborn as sns . Data Cleaning . Following the previous blog, the raw data we scraped from Amazon look like below. . Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my github notobooks . Below is data after cleaning. It contains 6 columns and more than 500 rows. . . EDA . Below are the questions I curioused about, and the result generated by doing data analysis. . Which rating (1-5) got the most and least? . | Which country are they targeting? . | Which month people prefer to give a higher rating? . | Which month people leave commons the most? . | What are the useful words that people mentioned in the reviews? . | . . Sentiment Analysis . What is sentiment analysis? . Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as “somewhat positive” and “somewhat negative.” . In this specific case, we catogrize 4 and 5 stars to the positive group and 1 &amp; 2 stars to the negative gorup. . . Below are the most frequently words in reviews from positive group and negative group respectively. . Positive review . Negative review . Build up the first model . Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative. . Because this is a classification task, we will train a simple logistic regression model. . Clean Data First, we create a new function to remove all punctuations from the data for later use. | . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final . Split the Dataframe | . Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews. . . df[&#39;random_number&#39;] = np.random.randn(len(index)) train = df[df[&#39;random_number&#39;] &lt;= 0.8] test = df[df[&#39;random_number&#39;] &gt; 0.8] . Create a bag of words | . Here I would like to introduce a new package. . Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. . In this example, we are going to use sklearn.feature_extraction.text.CountVectorizer to convert a collection of text documents to a matrix of token counts. . The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text. . train_matrix = vectorizer.fit_transform(train[&#39;title&#39;]) test_matrix = vectorizer.transform(test[&#39;title&#39;]) . Import Logistic Regression from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . | Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] . | Fit model on data lr.fit(X_train,y_train) . | Make predictionsa predictions = lr.predict(X_test) . | . The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa. . Testing . Now, we can test the accuracy of our model! . from sklearn.metrics import confusion_matrix, classification_report new = np.asarray(y_test) confusion_matrix(predictions,y_test) print(classification_report(predictions,y_test)) . . The accuracy is as high as 89%! .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping2.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping2.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Scraping Amazon Reviews using Scrapy in Python Part 1",
            "content": "Let’s get started! . . Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. . In this blog, we’ll look at scraping Amazon reviews with Python’s Scrapy. Scrapy is a python web crawling framework, and web scraping is a simple way of collecting data from many websites. Web scraping enables users to manage data for their specific needs, such as online merchandising, price monitoring, and marketing decision-making. . If you’re wondering whether or whether this procedure is legal, you can discover the answer here. . Before we get into scanning Amazon for product reviews, let’s take a look at some of the reasons why you might want to scrape Amazon reviews in the first place. . What exactly is Scrapy? . Scrapy is a web crawling framework that allows a developer to write code that defines how a specific site (or a group of websites) will be scraped. The most important feature is that it is built on Twisted, an asynchronous networking library, which significantly improves spider performance. . Why is it needed to scrape Amazon reviews? . Product reviews were subjected to a sentiment analysis. The reviews scraped from Amazon products can be subjected to sentiment analysis. This type of research aids in determining a user’s emotional response to a certain product. This can assist sellers, as well as other potential customers, in gaining a better understanding of public opinion about the goods. . | Dropshipping sales optimization Dropshipping is a business model that allows a company to operate without an inventory or a depository for product storage. You can utilize web scraping to acquire product pricing, user opinions, consumer wants, and trend information. . | Web scraping is used to keep track of a brand’s online reputation. It is tough for major corporations to keep track of their product reputation. Web scraping can aid in the extraction of pertinent review data, which can then be fed into various analysis tools to gauge user attitude toward the company. . | . The overall view . Examining the webpage’s HTML structure Scraping is the process of identifying patterns in web pages and extracting them. Before we begin writing a scraper, we must first understand the HTML structure of the target web page and identify patterns in it. The pattern is related to the repetitive use of classes, ids, and other HTML elements. . | Python scrapy parser implementation We work on the coded implementation in Python after analyzing the structure of the target web page. Scrapy parser’s responsibility is to visit the targeted web page and extract the information according to the rules specified. . | Information Gathering and Storage The parser can output the results in any format you want, including CSV and JSON. This is the final output where your scraped data is stored. . | . Shure MV7 . The product review we are going to scrap is the lastest dual USB;XLR microphone called Shure MV7. . . Shure is probabily the most well-known micphone company that Michael Jackson trusted. . . Examining the webpage’s HTML structure . At the very beginning, let’s first find some patterns from Amazon review page so that we can scrape it efficiently. . URL trick . The link below is the url of the review page. As you can see, it is complicated, and we cannot really capture a pattern from it easily. . https://www.amazon.com/Shure-Microphone-Podcasting-Voice-Isolating-Technology/product-reviews/B08G7RG9ML/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews . In actualality, the url can be shorten as below. . https://www.amazon.com/product-reviews/B08G7RG9ML?th=1 . THis looks clearer now. We go the the Amazon’s product-reviews subpage, then it follows by an ASIN code where you can find it from amazon product information. Copy it! . . Eventually, the integer “1” on the url represents the first page of the review pages. We can change it to 2, 3, … until the last page to check all reviews. . Inspect . Even though this looks complicated, but once we capture some patterns from inspection, we can easily get the data we need in a very efficient way. Here are some very fundamental knowledges about inspections that we are going to uitlize later on. . . To begin, we open the web page in the browser and use the inspect-element feature to inspect the elements. The HTML code for the web page can be found there. After some investigation, I discovered the following HTML structure, which renders the reviews on the web page. . . There is a section on the reviews page with the id cm cr-review list. This division contains multiple sub-divisions where the review content is located. We intend to extract rating stars as well as review comments from the web page. To prepare a scheme for retrieving both star ratings and review comments, we need to go one level deeper into one of the other sub-divisions. . . Upon closer examination, we can see that each review subdivision is further subdivided into multiple blocks. One of these blocks contains the required star ratings, while the others include the review text. When we look closer, we can see that the rating star division is represented by the class attribute “review-rating,” and review texts are represented by the class “review-text.” All we have to do now is use our Scrapy parser to pick up these patterns. . Python scrapy parser implementation . Install “scrapy” package . First, let’s install the scrapy package by enter the line below. . pip install scrapy . Then to create a scrapy project use following command. . scrapy startproject amazon_reviews_scraping . After entering the code, you should see these two. One is a folder that contains your scrapy code, and the other is your spacy configuration file. Spacy configuration while helps in running and deploying the Scrapy project on a server. . . We’ll need to creat a spider once we have a project in place. A spider is a piece of Python code that controls how a web page is scraped. It is the main component that crawls various web pages and extracts content from them. In our case, this will be the code chuck that will visit Amazon and scrape Amazon reviews. You can use the following command to make a spider. . scrapy genspider amazon_review your-link-here . The structure is like the image below. . . Files description . items.py Items are containers that will be loaded with the scraped data. . | Middleware.py The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to handle the requests and items that are generated from spiders. . | Pipelines.py After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially. Each item pipeline component is a Python class. . | settings.py It allows one to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. . | spiders folder The Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider, then scrapy looks into this directory and tries to find the spider with its name provided by the user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. . | . Here is the deeper explaination about each files. . Defining Scrapy Parser in Python . Now we go to the spider folder and open the file reviews.py. In this process, we create two variables which are reviews_url and asin_list for convience later. Then we create a new function to request the website. . . Then we download another package called scraper-helper to make the process even easier. . pip install scraper-helper . Now we go to folder Amazon_reviews_scraping and open settings.py. We delete everything but these lines. . . We use the new import library method “get_dict” to convert the string lines from comments to dictions format. You can simply copy and paste this piece because this process required some html knowledge. After this, we fit the request header and can successfully connect to the website. . The next step is to go back the review.py and finish the function “parse”. We open the inspect tool and can easily find that each star rating, data to review, and comments about the preduct has their own type unique headers, such as “[data-hook=”review-star-rating”]” for star rating. We find each unique header and get each type of information we need. . . Of course, after examining the css file, we can create a few lines to make sure after we scrape the date, it jump to the next page automatically and make sure we can collect enought data we need. . . Eventually, we run the code below by using terminal. . scrapy crawl reviews -o new_file_name.csv . This will generate a new csv file to help us to store the data we request from Amazon. . Conclusion . We did it! . Even though the whole process looks easy, but there are still some unexpected things that are warting for you to explore. For example, for star rating, different countries have different headers like below. . United States: . [data-hook=”review-star-rating”] . Other countries: . [data-hook=”cmps-review-star-rating”] . Another difficulty in scraping Amazon reviews is that Amazon tends to block IPs if you scrape Amazon frequently. This can be an impediment to your work. . In such cases, make sure to rotate your IP addresses on a regular basis and make fewer requests to the Amazon server to avoid being blocked. More information can be found here. . In the next blog, part 2, we are going to clean up the data and dig in the sentiment analysis. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/publication/jupyter/2022/03/07/scraping1.html",
            "relUrl": "/publication/jupyter/2022/03/07/scraping1.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Predicting Stock Change With Python",
            "content": "Stock is a highly sensitive and turbulent market. Because of the recent crisis between Russia and Ukraine, for example, a few comments from Putin or another powerful figure might lead millions of people to lose or make profit in a matter of minutes. A essential skill that modern people should have is the ability to foresee trends in order to preserve their investments and maximize their profits. In this blog, we are going to introduce three basic functions to support you to achieve the following goals. . First, selenium is the first option to support us to do web scraping from Yahoo finance based on the filter we set up, such as Aggressive Small Caps. Here is a Youtube selenium tutoria that I recommend you to set up selenium. And use this selenium python tutorial as more detailed reference. Then, we will get the historical data of that most active stock. Second, we will predict the stock trends. Eventually, we send out the predictions and the lastest change on the stock to our receiver by email. . . Set up the envirnment . Bleow are some packages that are necessary to run the code. . import pandas as pd import numpy as np import re from getpass import getpass from datetime import datetime, date, time, timezone import smtplib from selenium import webdriver import os #For Prediction from sklearn.linear_model import LinearRegression from sklearn import preprocessing from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from selenium.webdriver.common.by import By from selenium.webdriver.chrome.service import Service #For Stock Data from iexfinance.stocks import get_historical_data from iexfinance.stocks import Stock . Do not have enough stock data? . Method one: scrapying the stock data from Yahoo for free! . After following the tutorial at the very beginning of this blog, we can create our chrome drive and use driver.get(url) navigate to our desired webpage: Yahoo finance which displays the top 25 most active stocks by default. You can also change the filter based on what you are looking for. Inside webdriver.Chrome() you will need to type your chromedriver path. . After connecting to Yahoo by using webdriver, we could use a double for loops to crawl the whole database on the website. In detail, &quot;j&quot; represents how many rows we need, and &quot;i&quot; represents which column we need. . def getStocks(n): #Navigating to the Yahoo stock screener driver = webdriver.Chrome(service= Service( &#39;/Users/zeyu/Desktop/chromedriver&#39;)) url = &quot;https://finance.yahoo.com/screener/predefined/aggressive_small_caps?offset=0&amp;count=202&quot; driver.get(url) #Creating a stock list and iterating through the ticker names on the stock screener list data = [[] for i in range(9)] for i in range(1,len(data)+1): for j in range(1,n+1): ticker = driver.find_element(By.XPATH, &#39;//*[@id = &quot;scr-res-table&quot;]/div[1]/table/tbody/tr[&#39; + str(j) + &#39;]/td[&#39; + str(i) + &#39;]&#39;) data[i-1].append(ticker.text) driver.quit() #Using the stock list to predict the future price of the stock a specificed amount of days number = 0 for i in data: print(&quot;Number: &quot; + str(number)) try: predictData(i, 5) except: print(&quot;Stock: &quot; + i + &quot; was not predicted&quot;) number += 1 if __name__ == &#39;__main__&#39;: getStocks(20) . Method two: applying historical stock data from IEX Cloud with cost. . Before everything, you need to visit iexcloud to create an account and get a exclusive API. The free version only offers a very limited access. . Now, we can using the get_historical_data() package to get cleaned up dataset you want. Then there are a few parameters you need to enter. First, you need to enter the stock symblo like (AAPL). Then, set the start and end date, the output format (we will use pandas in this project), and eventually, the token which is API you acquired from the iexcloud website. . start = datetime(2021, 2, 17) end = datetime(2022, 2, 16) API = getpass(&quot;Please enter your API&quot;) df = get_historical_data(stock_symblo, start=start, end=end, output_format=&#39;pandas&#39;, token = API) . df = pd.read_csv(&#39;/Users/zeyu/Desktop/DS/Stock/Selenium/SPOT_df.csv&#39;) df = df.drop([&#39;subkey&#39;], axis = 1) pd.set_option(&#39;display.max_columns&#39;, None) df = df.rename(columns = {&quot;Unnamed: 0&quot; : &quot;date&quot;}) df.head() . container = [] for i in range(len(df.date)): x = re.sub((&quot;-&quot;), &quot;&quot;, df.date[i]) container.append(x) df.date = container df = df.drop([&#39;symbol&#39;, &quot;id&quot;, &quot;key&quot;, &quot;label&quot;], axis = 1) . Predict the future stock! . In the following lines, we first create a prediction column called &quot;Prediction&quot;, each day&#39;s close price is the prediction of the previous day. Second, two datasets are created. X is the predictor variable, Y is the target variable. Preprocessing package is to nomalize our predictor variables. Then we split our data into train and test datasets for both X and Y. Then we use the Regression on the training data, then predict the closing price of X_prediction. . def predictData(stock, days): if os.path.exists(&#39;./Exports&#39;): csv_name = (&#39;Exports/&#39; + stock + &#39;_Export.csv&#39;) else: os.mkdir(&quot;Exports&quot;) csv_name = (&#39;Exports/&#39; + stock + &#39;_Export.csv&#39;) df.to_csv(csv_name) df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) df.dropna(inplace = True) forecast_time = int(days) #Predicting the Stock price in the future X = np.array(df.drop([&#39;prediction&#39;], axis=1)) Y = np.array(df[&#39;prediction&#39;]) #Nomalize our predictor variables X = preprocessing.scale(X) print(&quot; nX aftre preprocessing:&quot;, X) X_prediction = X[-forecast_time:] # Split our data into train and test data X_train, X_test, Y_train, Y_Test = train_test_split(X, Y, test_size=0.5) print(&quot; n nX train:&quot;, X_train, &quot; nX_test:&quot;,X_test, &quot; nY_train:&quot;, Y_train, &quot; nY_Test&quot;, Y_Test) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) #Predict the closing price of X_prediction. prediction = (clf.predict(X_prediction)) last_row = df.tail(1) print(&quot;Last row:&quot;, last_row) print(&quot;Last row 1:&quot;, last_row[&#39;close&#39;]) #Sending the SMS if the predicted price of the stock is at least 1 greater than the previous closing price if (float(prediction[4]) &gt; (float(last_row[&#39;close&#39;])) + 1): output = (&quot; n nStock:&quot; + str(stock) + &quot; nPrior Close: n&quot; + str(last_row[&#39;close&#39;]) + &quot; n nPrediction in 1 Day: &quot; + str( prediction[0]) + &quot; nPrediction in 5 Days: &quot; + str(prediction[4])) #sendMessage(output) print(&quot;This is the output:&quot;, output) predictData(df, 5) . Magic that send message through gmail! . Eventually, the last function sendMessage(text) is for us to send messages through gamil when some conditions are triggered by using smtplib module. getpass() is another package use here for user enter the password to improve the security. . def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = getpass(&quot;Please enter your gmail address like chrisguan912@gmail.com&quot;) password = getpass(&quot;Please enter your password&quot;) vtext = getpass(&quot;Please enter the receiver&#39;s phone number and company&#39;s domain like 9292649156@txt.att.net&quot;) message = getpass(&quot;Please enter your message&quot;) MSG = &quot;&quot;&quot;From: %s n To: %s n %s &quot;&quot;&quot; %(username, vtext, message) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, vtext, MSG) server.quit() print(&#39;Sent&#39;) . Conclusion . The blog introduced four functions in total. The first two can support us to apply valuable data by using selenium or download from iexcloud. The third funciton is to analyze the predict the stock trend. The last few lines support us to send a message through gmail. .",
            "url": "https://christopherguan.github.io/sample-ds-blog/selenium/python/iex/yahoo/2022/02/24/Predict.html",
            "relUrl": "/selenium/python/iex/yahoo/2022/02/24/Predict.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "Resume",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/02/09/Resume.html",
            "relUrl": "/2020/02/09/Resume.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Abou Me",
            "content": "­­EDUCATION . Fei Tian College Middletown, Middletown, NY . Bachelor of Science, Data Science Expected June 2023 . No.2 Experimental High School, Changchun, Jilin . Graduated May 2015 . Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . EXPERIENCE . Youtube Data Automation December 2021 – January 2022 . Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. . Cleaned and stored data to MySQL database by using python. . Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. . CRM System May 2021– September 2021 . Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. . Implemented and applied requirements from financial department to four popular applications. . Created multiple video tutorials to help the future employees to understand the application better. . Data Mining January 2021 – May 2021 . Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. . Master on Pandas and Numpy python packages. . Developed movie recommendation system based on both content-based and collaborative filtering. . Business Data Analytics May 2020 – August 2020 . NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. . | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. . | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. . | . Database System January 2020 – May 2020 . Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. . Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. . Statistical Computing and Graphics . World Happiness Analysis September 2020 – December 2020 . Performed data manipulation and data cleaning. . | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. . | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. . | Analyzed and predicted the causation. . | . SKILLS . Computer/ Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. . Analytical skills, qualitative and quantitative research skills. . Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . CERTIFICATIONS . Data Analysis in Spreadsheets . Intermediate R for Finance . Credit Risk Modeling in R .",
            "url": "https://christopherguan.github.io/sample-ds-blog/2020/01/28/Abou-Me.html",
            "relUrl": "/2020/01/28/Abou-Me.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . I may be a hypocrite, but I’m not wrong… . Example Markdown Post . . I’ve spent over 25 years preaching the value of a portfolio site to artists, software engineers, and designers that wanted to join my studio or learn how to get into games or related industries. Now that I’m 6 months into my PhD and planning for career 2.0, it’s past time for me to do the same. This sums up the justification in a single tweet: . The popularity of this tweet makes me laugh, because it&#39;s an illustrative example! I thought about trying to get a screenshot of David&#39;s slide, but just took a terrible phone photo of a webinar and tweeted with the quote. . &mdash; Amelia McNamara (@AmeliaMN) July 22, 2018 I recently finished reading Build a Career in Data Science2, which I recommend. It motivated me to start the process of sharing my work. . Introduction to Fastpages . This site is built with fastpages, a platform for building static Jekyll web blogs from Jupyter Notebooks (JNs). Markdown and Word DOCs are also supported. . . From their github page: . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. . Fastpages is built by the team that created the deep learning PyTorch front-end fastai and the Python programming environment nbdev. The team is led by Jeremy Howard, former President and Chief Data Scientist of Kaggle and author of Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD1. . I was first introduced fastpages and nbdev during the recent ACM TechTalk, It’s Time Deep Learning Learned from Software Engineering, presented by Howard and moderated by his collaborator Hamel Husain. . From Introducing Fastpages: . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . Some resources I found helpful in launching this blog: . The fastpages github page has detailed setup instructions | This video tutorial created by Abdul Majed walks you through the initial setup process | A sample jupyter-based blog page provides a live demonstration of its capabilities | . I was able to get the initial site live quite quickly with little drama, and have found the process of modifying existing content and publishing new material straightforward so far. Before choosing fastpages I considered the following hosting / authoring options: . A general purpose site like Wix | A page on Medium or Substack | Other JN-based tools like Pelican | Finding a way to use JNs in blogdown3 | . I decided that Fastpages was the best way for me to publish JN-based work directly, with professional look and feel, on a site that I control, for free. That’s a pretty great combination. I also like that it supports Markdown (like this page), is part of the larger fast.ai family of products (and philosophy), and is hosted on github. . Finally, if you still need to be convinced of the value of building a portfolio, and what to include in one give the following resources a look: . Advice to aspiring data scientists: start a blog on David Robinson’s Variance Explained | Data Science Portfolios That Will Get You the Job | Thinking of blogging about Data Science? Here are some tips and possible benefits. | . Many of us need to spend less time consuming, and more time creating. Now you know how and why, so go publish something! . . Howard, Jeremy, and Sylvain Gugger. Deep Learning for Coders with Fastai and PyTorch. O’Reilly Media, Inc., 2020. &#8617; &#8617;2 . | Robinson, Emily, and Jacqueline Nolis. Build a Career in Data Science. Simon and Schuster, 2020. &#8617; . | There is a lot of cool stuff going on in R these days and blogdown supports a lot of interesting publishing workflows, but I ultimately decided that it would probably be easier to find a way to publish my R work using fastpages than to publish my Python work with blogdown. Time will tell… &#8617; . |",
            "url": "https://christopherguan.github.io/sample-ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Chris",
          "content": ". Courses . Data Mining, Cloud Computing and Big data, Business Data Analytics, Statistical Computing and Graphics, Database Systems, Data structure and Algorithm, Computation Analysis and Practical Programing, Linear Algebra, Probability, Statistics . Experience . Youtube Data Automation                                                              December 2021 – January 2022 Paid Project . Obtained authorization credentials and stored hidden Youtube data in a csv file. | Cleaned and stored data to MySQL database by using python. | Developed an application that execute all previous actions and displayed monthly report on google sheet periodically and automatically. | . . CRM System                                                                                         May 2021– September 2021 Summer Internship . Narrowed CRM applications that fit the financial department’s expectation. | Implemented and applied requirements from financial department to four popular applications. | Created multiple video tutorials to help the future employees to understand the application better. | . . Data Mining                                                                                               January 2021 – May 2021 Multiple Projects . Outlined and understood the overall view of data mining through learning and proficient use of RapidMiner Studio. | Master on Pandas and Numpy python packages. | Developed movie recommendation system based on both content-based and collaborative filtering. | . . Business Data Analytics                                                                           May 2020 – August 2020 NYC Arrest Record . Imported, cleaned, and transformed data involving NYC arrest record by using Microsoft Excel and Tableau. | Designed, created and developed interactive dynamic dashboard for NYC arrest record by using Tableau to better visualize, diagnose, and predict abnormal fluctuations in the number of arrests and countermeasures. | Exhibited, informed, and interpreted valuable informative outputs of the exploratory data analysis and the prediction of the safest area to live, the change in number of the arrest, and the life characteristics of different races. | . . Database System                                                                                      January 2020 – May 2020 Website and SQL Queries . Used Python flask and HTML, ran on AWS EC2, made a friendly front-end UI. | Wrote several SQL Queries to extract informative results to satisfy passengers’ understanding of flights and airports. | . . Statistical Computing and Graphics                                        September 2020 – December 2020 World Happiness Analysis . Performed data manipulation and data cleaning. | Analyzed and normalized the data, and displayed graphs based on the analysis using RStudio, with the tidyverse and ggplot2 packages. | Demoed and explained the disclosures including the outliers and the irregular fluctuations in the happiness ranking. | Analyzed and predicted the causation. | . . Skills . Computer Tools: Tableau, AWS, Python, R, MySQL Workbench, RapidMiner, Advanced knowledge of Excel, Intermediate level of Java, HTML, CSS. Analytical skills, qualitative and quantitative research skills. Public Speaking; Public Relations; Event Planning, Storyteller, Teamwork; Detail-oriented; Social skills; Project management . . Certifications . Data Analysis in Spreadsheets | Intermediate R for Finance | Credit Risk Modeling in R | . . Favorite Quote . “Great beauty, great strength, and great riches are really and truly of no great use; a right heart exceeds all” ― Benjamin Franklin .",
          "url": "https://christopherguan.github.io/sample-ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://christopherguan.github.io/sample-ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}