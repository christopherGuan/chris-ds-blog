---
toc: true
layout: post
description: "Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews. "
categories: [publication, jupyter]
title: "Scraping Amazon Reviews using Scrapy in Python Part 2"
image: "https://www.freecodecamp.org/news/content/images/2020/09/wall-5.jpeg"
hide: true
comments: true
---

## Required Packages
[wordcloud](https://github.com/amueller/word_cloud), 
[geopandas](https://geopandas.org/en/stable/getting_started/install.html), 
[nbformat](https://pypi.org/project/nbformat/), 
[seaborn](https://seaborn.pydata.org/installing.html), 
[scikit-learn](https://scikit-learn.org/stable/install.html)


## Now let's get started!
First thing first, you need to load all the necessary libraries:
```python
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
from wordcloud import WordCloud
from wordcloud import STOPWORDS
import re
import plotly.graph_objects as go
import seaborn as sns
```

# Data Cleaning

Following the previous blog, the raw data we scraped from Amazon look like below. 
![Raw Data](https://article.images.consumerreports.org/f_auto/prod/content/dam/CRO%20Images%202019/Electronics/08August/CR-Electronics-inline-Bv1-amazon-review-hacking-0819)

Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my [github notobooks](https://github.com/christopherGuan/sample-ds-blog)

Below is data after cleaning. It contains 6 columns and more than 500 rows. 

![Clean Data](https://live.staticflickr.com/65535/51930037691_4a23b4c441_b.jpg)


# EDA
Below are the questions I curioused about, and the result generated by doing data analysis.

- Which rating (1-5) got the most and least?
![Which point was rated the most](https://live.staticflickr.com/65535/51929072567_d34db66693_h.jpg)

- Which country are they targeting?
![Target Country](https://live.staticflickr.com/65535/51930675230_6314e2ccde_h.jpg)

- Which month people prefer to give a higher rating?
![higher rating](https://live.staticflickr.com/65535/51929085842_0cb0aa6b06_w.jpg)

- Which month people leave commons the most?
![More commons](https://live.staticflickr.com/65535/51929085857_49f7c889d2_w.jpg )


- What are the useful words that people mentioned in the reviews?

![More commons](https://live.staticflickr.com/65535/51930471329_82bf0c43b9.jpg)

# Sentiment Analysis

## What is sentiment analysis?

Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as "somewhat positive" and "somewhat negative." 

In this specific case, we catogrize 4 and 5 stars to the positive group and 1 & 2 stars to the negative gorup. 

![rating](https://live.staticflickr.com/65535/51930237493_b6afc18052_c.jpg)

Below are the most frequently words in reviews from positive group and negative group respectively. 

Positive review
![positive](https://live.staticflickr.com/65535/51930164126_33b911e6b3_c.jpg)

Negative review
![negative](https://live.staticflickr.com/65535/51930165221_cf61fce68e_c.jpg)

## Build up the first model

Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative.

Because this is a classification task, we will train a simple logistic regression model.


- **Clean Data**
First, we create a new function to remove all punctuations from the data for later use.

```python
def remove_punctuation(text):
    final = "".join(u for u in text if u not in ("?", ".", ";", ":",  "!",'"'))
    return final
```

- **Split the Dataframe**

Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews.

![output](https://live.staticflickr.com/65535/51930294148_3a9db0297c_b.jpg)

```python
df['random_number'] = np.random.randn(len(index))
train = df[df['random_number'] <= 0.8]
test = df[df['random_number'] > 0.8]
```


- **Create a bag of words**

Here I would like to introduce a new package.

[Scikit-learn](https://scikit-learn.org/stable/install.html) is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.

In this example, we are going to use [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) to convert a collection of text documents to a matrix of token counts.

The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text.

```python
train_matrix = vectorizer.fit_transform(train['title'])
test_matrix = vectorizer.transform(test['title'])
```

- **Import Logistic Regression**
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
```

- **Split target and independent variables**
```python
X_train = train_matrix
X_test = test_matrix
y_train = train['sentiment']
y_test = test['sentiment']
```

- **Fit model on data**
```python
lr.fit(X_train,y_train)
```

- **Make predictionsa**
```python
predictions = lr.predict(X_test)
```

The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa.

### Testing

Now, we can test the accuracy of our model!

```python
from sklearn.metrics import confusion_matrix, classification_report
new = np.asarray(y_test)
confusion_matrix(predictions,y_test)

print(classification_report(predictions,y_test))
```

![accuracy](https://live.staticflickr.com/65535/51929260132_045027628c_z.jpg)

The accuracy is as high as 89%!

































