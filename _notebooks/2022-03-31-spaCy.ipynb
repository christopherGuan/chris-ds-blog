{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing spaCy For NLP\n",
    "> This spaCy tutorial explains and applies the introduction to spaCy and features of spaCy for NLP based on the [spaCy 101](https://spacy.io/usage/spacy-101).\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Zeyu Guan\n",
    "- categories: [spaCy, Python, Machine Learning, Data Mining, NLP, RandomForest]\n",
    "- annotations: true\n",
    "- image: https://spacy.io/static/social_default-1d3b50b1eba4c2b06244425ff0c49570.jpg\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, what is spacy?\n",
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python that allows you to perform extensive natural language processing analysis and develop models that may be used to support document analysis, chatbot capabilities, and other types of text analysis.\n",
    "\n",
    "It's become one of the most extensively used natural language libraries in Python for corporate use cases, and it has a sizable community—and with it, a lot of support for commercializing research breakthroughs as this field evolves rapidly.\n",
    "\n",
    "After downloading a list of packages, we can load spaCy and run some code like below. That nlp variable, which is loaded with the en_core_web_sm small model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run a small \"document\" through the natural language parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "doc = nlp(\"He went to play basketball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names\n",
    "nlp.disable_pipes('tagger', 'parser')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Part-of-Speech (POS) Tagging using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token.text, \"-->\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dependency Parsing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency parsing\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Named Entity Recognition using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Today is March 21, and Chris has already spent a few hours on this project in the United States.\")\n",
    " \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rule-Based Matching using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the spaCy vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Some people start their day with lemon water\")\n",
    "\n",
    "# Define rule\n",
    "pattern = [{'TEXT': 'lemon'}, {'TEXT': 'water'}]\n",
    "\n",
    "# Add rule\n",
    "matcher.add('rule_1',[pattern])\n",
    "\n",
    "print(matcher(doc))\n",
    "\n",
    "matches = matcher(doc)\n",
    "matches\n",
    "\n",
    "# Extract matched text\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"You read this book\")\n",
    "doc2 = nlp(\"I will book my ticket\")\n",
    "\n",
    "#Text and POS are required\n",
    "pattern = [{'TEXT': 'book', 'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('rule_2', [pattern])\n",
    "\n",
    "\n",
    "print(\"doc1: \", matcher(doc1),\"\\ndoc2: \", matcher(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let’s get our hands dirty with spaCy.\n",
    "### First, let's play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process as above.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The rain in Spain falls mainly on the plain.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# stoplist\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got many valuable information by using the package, but it's a bit difficult to read. Thus, we reformat the spaCy parse of that sentence as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "for t in doc:\n",
    "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the displaCy library to visualize the parse tree for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the this only work on a single sentence. What if we have more sentences like a novel? There are features for sentence boundary detection (SBD)—also known as sentence segmentation—based on the builtin/default sentencizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in.Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\">\", sent.start, sent.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[48:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[51]\n",
    "print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring Text\n",
    "\n",
    "Using the internet as a quick source is one option for us to acquire texts. Of course, we get HTML when we download web pages and must subsequently extract text from them. For this, Beautiful Soup is a popular package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import traceback\n",
    "def get_text (url):\n",
    "    buf = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            buf.append(p.get_text())\n",
    "        return \"\".join(buf)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic = {}\n",
    "lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\n",
    "lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-20\"))\n",
    "lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n",
    "\n",
    "for sent in lic[\"bsd\"].sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\"mit\", \"asl\"],\n",
    "    [\"asl\", \"bsd\"],\n",
    "    [\"bsd\", \"mit\"]]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(a, b, lic[a].similarity(lic[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting since the BSD and MIT licenses appear to be the most similar documents. In fact, they are closely related. There are many applications in real-life experience, such as distinguishing an article's opinions and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "\n",
    "print(\"before\", nlp.pipe_names)\n",
    "\n",
    "#V3 difference\n",
    "if \"spacy_wordnet\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "else:\n",
    "    print(\"after\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"withdraw\")[0]\n",
    "token._.wordnet.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.wordnet.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.wordnet.wordnet_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"finance\", \"banking\"]\n",
    "sentence = nlp(\"I want to withdraw 5,000 euros.\")\n",
    "\n",
    "enriched_sent = []\n",
    "\n",
    "for token in sentence:\n",
    "    # get synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
    "\n",
    "    if synsets:\n",
    "        lemmas_for_synset = []\n",
    "\n",
    "        for s in synsets:\n",
    "        # get synset variants and add to the enriched sentence\n",
    "            lemmas_for_synset.extend(s.lemma_names())\n",
    "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
    "    else:\n",
    "        enriched_sent.append(token.text)\n",
    "print(\" \".join(enriched_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Comparison with spaCy and Scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "\n",
    "if \"merge_entities\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe('merge_entities')\n",
    "\n",
    "if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "convention_df = st.SampleCorpora.ConventionData2012.get_data()\n",
    "corpus = st.CorpusFromPandas(convention_df, category_col=\"party\", text_col=\"text\", nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus, category=\"democrat\", \n",
    "                                       category_name=\"Democratic\",\n",
    "                                       not_category_name=\"Republican\",\n",
    "                                       width_in_pixels=1000,\n",
    "                                       metadata=convention_df[\"speaker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"foo.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
