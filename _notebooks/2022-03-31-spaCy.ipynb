{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing spaCy For NLP\n",
    "> This spaCy tutorial explains and applies the introduction to spaCy and features of spaCy for NLP based on the [spaCy 101](https://spacy.io/usage/spacy-101).\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Zeyu Guan\n",
    "- categories: [spaCy, Python, Machine Learning, Data Mining, NLP, RandomForest]\n",
    "- annotations: true\n",
    "- image: https://spacy.io/static/social_default-1d3b50b1eba4c2b06244425ff0c49570.jpg\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, what is spacy?\n",
    "SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python that allows you to perform extensive natural language processing analysis and develop models that may be used to support document analysis, chatbot capabilities, and other types of text analysis.\n",
    "\n",
    "This blog should cover a few features you need to know about spaCy, whether you're new to it or just want to brush up on some NLP basics and implementation specifics.\n",
    "\n",
    "It's become one of the most extensively used natural language libraries in Python for corporate use cases, and it has a sizable community—and with it, a lot of support for commercializing research breakthroughs as this field evolves rapidly.\n",
    "\n",
    "After downloading a list of packages, we can load spaCy and run some code like below. That nlp variable, which is loaded with the en_core_web_sm small model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run a small \"document\" through the natural language parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "doc = nlp(\"He went to play basketball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nlp.pipe_names** shows spaCy’s processing pipeline. The NLP pipeline has multiple components, such as tokenizer, tagger, parser, ner, etc. So, the input text string has to go through all these components before we can work on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just in case you wish to disable the pipeline components and keep only the tokenizer up and running, then you can use the code **nlp.disable_pipes** to disable the pipeline components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.disable_pipes('tagger', 'parser')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, you will learn to perform various NLP tasks using spaCy. We will start off with the popular NLP tasks of Part-of-Speech Tagging, Dependency Parsing, and Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Part-of-Speech (POS) Tagging using spaCy\n",
    "In English, nouns, pronouns, adjectives, verbs, and adverbs are some of the most prevalent parts of speech. The task of automatically assigning POS tags to all the words in a sentence is known as POS tagging. It's useful for a variety of downstream NLP applications, including feature engineering, language comprehension, and information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He --> PRON\n",
      "went --> VERB\n",
      "to --> PART\n",
      "play --> VERB\n",
      "basketball --> NOUN\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token.text, \"-->\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dependency Parsing using spaCy\n",
    "\n",
    "Every sentence has a grammatical structure, which may be extracted with the assistance of dependency parsing. It can alternatively be viewed as a directed graph, with nodes corresponding to the words in the sentence and edges between nodes corresponding to the word dependencies. Performing dependency parsing in spaCy is simple as well. We'll use the same document we did for POS labeling here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He --> nsubj\n",
      "went --> ROOT\n",
      "to --> aux\n",
      "play --> advcl\n",
      "basketball --> dobj\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Named Entity Recognition using spaCy\n",
    "\n",
    "What entities are. Entities are words or collections of words that represent information about everyday objects like people, places, and organizations. These things have official names. Like the example below, Today is a date. Chris is assumed as a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today DATE\n",
      "March 21 DATE\n",
      "Chris PERSON\n",
      "a few hours TIME\n",
      "the United States GPE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Today is March 21, and Chris has already spent a few hours on this project in the United States.\")\n",
    " \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rule-Based Matching using spaCy\n",
    "\n",
    "It's hard to explain the meaning of Rule-Based matching using spacy; however, the output of the following code demenstrate the meaning cristal clear. With this spaCy matcher, you can find words and phrases in the text using user-defined rules.\n",
    "\n",
    "So, in the code below:\n",
    "\n",
    "- First, we import the spaCy matcher\n",
    "- After that, we initialize the matcher object with the default spaCy vocabulary\n",
    "- Then, we pass the input in an NLP object as usual\n",
    "- In the next step, we define the rule/pattern for what we want to extract from the text.\n",
    "\n",
    "Let's imagine we're looking for the term \"lemon water\" in a text. As a result, our goal is for the matcher to be able to discover this pattern in the text whenever \"lemon\" is followed by the word \"water.\" That's exactly what we did in the code above when defining the pattern. Finally, the defined rule is applied to the matcher object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7604275899133490726, 6, 8)]\n",
      "lemon water\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the spaCy vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Some people start their day with lemon water\")\n",
    "\n",
    "# Define rule\n",
    "pattern = [{'TEXT': 'lemon'}, {'TEXT': 'water'}]\n",
    "\n",
    "# Add rule\n",
    "matcher.add('rule_1',[pattern])\n",
    "\n",
    "print(matcher(doc))\n",
    "\n",
    "matches = matcher(doc)\n",
    "matches\n",
    "\n",
    "# Extract matched text\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above has three elements. The first element, ‘7604275899133490726’, is the match ID. The second and third elements are the positions of the matched tokens.\n",
    "\n",
    "Let's have a look at another application of the spaCy matcher. Consider the following two sentences:\n",
    "\n",
    "- You can read this book\n",
    "- I will book my ticket\n",
    "\n",
    "We are now interested in determining whether or not a sentence contains the word \"book.\" It appears to be quite simple, doesn't it? But here's the catch: we can only find the word \"book\" if it was used as a noun in the sentence.\n",
    "\n",
    "The word \"book\" was used as a noun in the first sentence and as a verb in the second sentence. As a result, the spaCy matcher should only be able to extract the pattern from the first sentence. Let's put it to the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1:  [(375134486054924901, 3, 4)] \n",
      "doc2:  []\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"You read this book\")\n",
    "doc2 = nlp(\"I will book my ticket\")\n",
    "\n",
    "#Text and POS are required\n",
    "pattern = [{'TEXT': 'book', 'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('rule_2', [pattern])\n",
    "\n",
    "print(\"doc1: \", matcher(doc1),\"\\ndoc2: \", matcher(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a quick overview to give you an idea of what spaCy can do. Trust me, you'll find yourself using spaCy frequently for NLP tasks. I encourage you to experiment with the code, download a dataset from DataHack, and try your hand at it with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let’s get our hands dirty with spaCy.\n",
    "### First, let's play around.\n",
    "\n",
    "That **nlp** variable is now your gateway to all things spaCy and loaded with the **en_core_web_sm** small model for English. Next, let's run a small \"document\" through the natural language parser.\n",
    "\n",
    "We started by making a doc out of the text, which is a container for a document and all of its annotations. Then we went through the document iteratively to see what spaCy had parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET True\n",
      "rain rain NOUN False\n",
      "in in ADP True\n",
      "Spain Spain PROPN False\n",
      "falls fall VERB False\n",
      "mainly mainly ADV False\n",
      "on on ADP True\n",
      "the the DET True\n",
      "plain plain NOUN False\n",
      ". . PUNCT False\n"
     ]
    }
   ],
   "source": [
    "# Same process as above.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The rain in Spain falls mainly on the plain.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# stoplist\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good, but there's a lot of information and it's a little difficult to read. Let's make the spaCy parse of that sentence into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>explain</th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rain</td>\n",
       "      <td>rain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>falls</td>\n",
       "      <td>fall</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mainly</td>\n",
       "      <td>mainly</td>\n",
       "      <td>ADV</td>\n",
       "      <td>adverb</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text   lemma    POS      explain  stopword\n",
       "0     The     the    DET   determiner      True\n",
       "1    rain    rain   NOUN         noun     False\n",
       "2      in      in    ADP   adposition      True\n",
       "3   Spain   Spain  PROPN  proper noun     False\n",
       "4   falls    fall   VERB         verb     False\n",
       "5  mainly  mainly    ADV       adverb     False\n",
       "6      on      on    ADP   adposition      True\n",
       "7     the     the    DET   determiner      True\n",
       "8   plain   plain   NOUN         noun     False\n",
       "9       .       .  PUNCT  punctuation     False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "for t in doc:\n",
    "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the displaCy library to visualize the [parse tree](https://en.wikipedia.org/wiki/Parse_tree#:~:text=A%20parse%20tree%20or%20parsing,to%20some%20context%2Dfree%20grammar.) for the sentence. If you are familier with parse tree, a parse tree, also known as a parsing tree, derivation tree, or concrete syntax tree, is a rooted tree that represents the syntactic structure of a string according to some context-free grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5e42e13aec4f47c2afc98862db1c9779-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">rain</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Spain</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">falls</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mainly</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">plain.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5e42e13aec4f47c2afc98862db1c9779-0-7\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5e42e13aec4f47c2afc98862db1c9779-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,266.5 L1453.0,254.5 1437.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the this only work on a single sentence. What if we have more sentences like a novel? There are features for sentence boundary detection (SBD)—also known as sentence segmentation—based on the builtin/default sentencizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.\n",
      "> I fell in.\n",
      "> Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.\n",
      "> The gorillas just went wild.\n"
     ]
    }
   ],
   "source": [
    "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in.Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When spaCy generates a document, it employs a non-destructive tokenization principle, which means that the tokens, sentences, and so on are simply indexes into a large array. In other words, they do not cut up the text stream into small chunks. As a result, each sentence is a span with a start and end index into the document array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0 25\n",
      "> 25 29\n",
      "> 29 48\n",
      "> 48 54\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\">\", sent.start, sent.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index into the document array to pull out the tokens for one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The gorillas just went wild."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[48:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply index into a specific token, such as the verb **went** in the last sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went go VERB\n"
     ]
    }
   ],
   "source": [
    "token = doc[51]\n",
    "print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can parse a document, segment that document into sentences, then look at annotations about the tokens in each sentence. That's a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring Text\n",
    "\n",
    "Using the internet as a quick source is one option for us to acquire texts. Of course, we get HTML when we download web pages and must subsequently extract text from them. For this, [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a popular package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function get_text() we'll parse the HTML to find all of the **\\<p/>** tags, then extract the text for those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import traceback\n",
    "def get_text (url):\n",
    "    buf = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            buf.append(p.get_text())\n",
    "        return \"\".join(buf)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab some text from online sources. We can compare open-source licenses hosted on the [Open Source Initiative](https://opensource.org/licenses/) site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SPDX short identifier: BSD-3-Clause Note: This license has also been called the \"New BSD License\" or \"Modified BSD License\".\n",
      "> See also the 2-clause BSD License.\n",
      "> Copyright <YEAR> <COPYRIGHT HOLDER>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:1.\n",
      "> Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.3.\n",
      "> Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
      "> THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\n",
      "> CONTRIBUTORS\n",
      "> \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n",
      "> IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n",
      "> HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.For over 20 years the Open Source Initiative (OSI) has worked to raise awareness and adoption of open source software, and build bridges between open source communities of practice.\n",
      "> As a global non-profit, the OSI champions software freedom in society through education, collaboration, and infrastructure, stewarding the Open Source Definition (OSD), and preventing abuse of the ideals and ethos inherent to the open source movement.\n",
      "> Open source software is made by many people and distributed under an OSD-compliant license which grants all the rights to use, study, change, and share the software in modified and unmodified form.\n",
      "> Software freedom is essential to enabling community development of open source software.\n",
      ">  Sign-up for our newsletter!The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License.\n",
      "> \n",
      "Opensource.org is not the author of any of the licenses reproduced on this site.\n",
      "> Questions about the copyright in a license should be directed to the license steward.\n",
      "> Hosting for Opensource.org is generously provided by DigitalOcean.\n",
      "> Please see Terms of Service.\n",
      "> For questions regarding the OSI website and contents please contact us.\n",
      ">  \n"
     ]
    }
   ],
   "source": [
    "lic = {}\n",
    "lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\n",
    "lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-20\"))\n",
    "lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n",
    "\n",
    "for sent in lic[\"bsd\"].sents:\n",
    "    print(\">\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text comparison is a common application for natural language work. With those open-source licenses, for example, we can download their text, parse it, and then compare similarity metrics among them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mit asl 0.7400606692738089\n",
      "asl bsd 0.7629310829561079\n",
      "bsd mit 0.976838684194463\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    [\"mit\", \"asl\"],\n",
    "    [\"asl\", \"bsd\"],\n",
    "    [\"bsd\", \"mit\"]]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(a, b, lic[a].similarity(lic[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intriguing because the BSD and MIT licenses appear to be the most similar. They are, in fact, closely related.\n",
    "\n",
    "Due to the OSI disclaimer in the footer, some extra text was included in each document—but this provides a reasonable approximation for comparing the licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding\n",
    "Now, let's look at some of the spaCy features for NLU. Given a parse of a document, we can extract the noun chunks, i.e., each of the noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs\n",
      "Steve Wozniak\n",
      "Apple Computer\n",
      "January\n",
      "Cupertino\n",
      "California\n"
     ]
    }
   ],
   "source": [
    "text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun phrases in a sentence generally provide more information content as a simple filter used to reduce a long document into a more \"distilled\" representation.\n",
    "\n",
    "This approach can be extended by identifying named entities within the text, i.e., proper nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs PERSON\n",
      "Steve Wozniak PERSON\n",
      "Apple Computer ORG\n",
      "January 3, 1977 DATE\n",
      "Cupertino GPE\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The displaCy library provides an excellent way to visualize named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " incorporated \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Computer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    January 3, 1977\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cupertino\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a spaCy integration for WordNet called [spacy-wordnet](https://github.com/recognai/spacy-wordnet) by [Daniel Vila Suero](https://twitter.com/dvilasuero), an expert in natural language and knowledge graph work.\n",
    "\n",
    "Then we'll load the WordNet data via NLTK (these things happen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zeyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that spaCy operates as a \"pipeline\" and provides means for customizing parts of the pipeline in use. This is fantastic for enabling really interesting workflow integrations in data science work. We'll include the WordnetAnnotator from the spacy-wordnet project here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "\n",
    "print(\"before\", nlp.pipe_names)\n",
    "\n",
    "#V3 difference\n",
    "if \"spacy_wordnet\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "else:\n",
    "    print(\"after\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('withdraw.v.01'),\n",
       " Synset('retire.v.02'),\n",
       " Synset('disengage.v.01'),\n",
       " Synset('recall.v.07'),\n",
       " Synset('swallow.v.05'),\n",
       " Synset('seclude.v.01'),\n",
       " Synset('adjourn.v.02'),\n",
       " Synset('bow_out.v.02'),\n",
       " Synset('withdraw.v.09'),\n",
       " Synset('retire.v.08'),\n",
       " Synset('retreat.v.04'),\n",
       " Synset('remove.v.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = nlp(\"withdraw\")[0]\n",
    "token._.wordnet.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('withdraw.v.01.withdraw'),\n",
       " Lemma('withdraw.v.01.retreat'),\n",
       " Lemma('withdraw.v.01.pull_away'),\n",
       " Lemma('withdraw.v.01.draw_back'),\n",
       " Lemma('withdraw.v.01.recede'),\n",
       " Lemma('withdraw.v.01.pull_back'),\n",
       " Lemma('withdraw.v.01.retire'),\n",
       " Lemma('withdraw.v.01.move_back'),\n",
       " Lemma('retire.v.02.retire'),\n",
       " Lemma('retire.v.02.withdraw'),\n",
       " Lemma('disengage.v.01.disengage'),\n",
       " Lemma('disengage.v.01.withdraw'),\n",
       " Lemma('recall.v.07.recall'),\n",
       " Lemma('recall.v.07.call_in'),\n",
       " Lemma('recall.v.07.call_back'),\n",
       " Lemma('recall.v.07.withdraw'),\n",
       " Lemma('swallow.v.05.swallow'),\n",
       " Lemma('swallow.v.05.take_back'),\n",
       " Lemma('swallow.v.05.unsay'),\n",
       " Lemma('swallow.v.05.withdraw'),\n",
       " Lemma('seclude.v.01.seclude'),\n",
       " Lemma('seclude.v.01.sequester'),\n",
       " Lemma('seclude.v.01.sequestrate'),\n",
       " Lemma('seclude.v.01.withdraw'),\n",
       " Lemma('adjourn.v.02.adjourn'),\n",
       " Lemma('adjourn.v.02.withdraw'),\n",
       " Lemma('adjourn.v.02.retire'),\n",
       " Lemma('bow_out.v.02.bow_out'),\n",
       " Lemma('bow_out.v.02.withdraw'),\n",
       " Lemma('withdraw.v.09.withdraw'),\n",
       " Lemma('withdraw.v.09.draw'),\n",
       " Lemma('withdraw.v.09.take_out'),\n",
       " Lemma('withdraw.v.09.draw_off'),\n",
       " Lemma('retire.v.08.retire'),\n",
       " Lemma('retire.v.08.withdraw'),\n",
       " Lemma('retreat.v.04.retreat'),\n",
       " Lemma('retreat.v.04.pull_back'),\n",
       " Lemma('retreat.v.04.back_out'),\n",
       " Lemma('retreat.v.04.back_away'),\n",
       " Lemma('retreat.v.04.crawfish'),\n",
       " Lemma('retreat.v.04.crawfish_out'),\n",
       " Lemma('retreat.v.04.pull_in_one's_horns'),\n",
       " Lemma('retreat.v.04.withdraw'),\n",
       " Lemma('remove.v.01.remove'),\n",
       " Lemma('remove.v.01.take'),\n",
       " Lemma('remove.v.01.take_away'),\n",
       " Lemma('remove.v.01.withdraw')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astronomy',\n",
       " 'school',\n",
       " 'telegraphy',\n",
       " 'industry',\n",
       " 'psychology',\n",
       " 'ethnology',\n",
       " 'ethnology',\n",
       " 'administration',\n",
       " 'school',\n",
       " 'finance',\n",
       " 'economy',\n",
       " 'exchange',\n",
       " 'banking',\n",
       " 'commerce',\n",
       " 'medicine',\n",
       " 'ethnology',\n",
       " 'university',\n",
       " 'school',\n",
       " 'buildings',\n",
       " 'factotum',\n",
       " 'agriculture',\n",
       " 'mechanics',\n",
       " 'gastronomy',\n",
       " 'meteorology',\n",
       " 'physics',\n",
       " 'basketball',\n",
       " 'anatomy',\n",
       " 'skiing',\n",
       " 'nautical',\n",
       " 'engineering',\n",
       " 'racing',\n",
       " 'home',\n",
       " 'drawing',\n",
       " 'dentistry',\n",
       " 'ethnology',\n",
       " 'mathematics',\n",
       " 'furniture',\n",
       " 'animal_husbandry',\n",
       " 'industry',\n",
       " 'economy',\n",
       " 'body_care',\n",
       " 'chemistry',\n",
       " 'medicine',\n",
       " 'surgery',\n",
       " 'vehicles',\n",
       " 'transport',\n",
       " 'atomic_physic',\n",
       " 'archaeology',\n",
       " 'hydraulics',\n",
       " 'oceanography',\n",
       " 'golf',\n",
       " 'sculpture',\n",
       " 'earth',\n",
       " 'applied_science',\n",
       " 'artisanship']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.wordnet_domains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if you're working with knowledge graphs, those WordNet \"word sense\" links could be used in conjunction with graph algorithms to help identify the meanings of specific words. This technique, known as summarization, can also be used to create summaries for longer sections of text. It's beyond the scope of this tutorial, but it's an interesting application for natural language in the industry right now.\n",
    "\n",
    "In the opposite direction, if you know ahead of time that a document is about a specific domain or set of topics, you can limit the meanings returned by WordNet. In the following example, we'll look at NLU results from the Finance and Banking sectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I (privation|want|neediness|deprivation) (deficiency|deprivation|privation|lack|want|neediness) (deficiency|deprivation|privation|require|lack|want|need|neediness) to (withdraw|draw_off|draw|take_out) 5,000 euros .\n"
     ]
    }
   ],
   "source": [
    "domains = [\"finance\", \"banking\"]\n",
    "sentence = nlp(\"I want to withdraw 5,000 euros.\")\n",
    "\n",
    "enriched_sent = []\n",
    "\n",
    "for token in sentence:\n",
    "    # get synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
    "\n",
    "    if synsets:\n",
    "        lemmas_for_synset = []\n",
    "\n",
    "        for s in synsets:\n",
    "        # get synset variants and add to the enriched sentence\n",
    "            lemmas_for_synset.extend(s.lemma_names())\n",
    "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
    "    else:\n",
    "        enriched_sent.append(token.text)\n",
    "print(\" \".join(enriched_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That example may appear straightforward, but if you experiment with the **domains** list, you'll notice that the results exhibit a kind of combinatorial explosion when run without reasonable constraints. Consider a knowledge graph with millions of elements: you'd want to limit searches wherever possible to avoid having each query take days/weeks/months/years to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Comparison with spaCy and Scattertext\n",
    "\n",
    "Sometimes the problems encountered when attempting to understand a text—or, better yet, attempting to understand a corpus (a dataset containing many related texts)—become so complex that they must first be visualized. Here's an interactive visualization to help you understand text: [Jason Kessler](https://twitter.com/jasonkessler)'s genius has resulted in [scattertext](https://spacy.io/universe/project/scattertext).\n",
    "\n",
    "Let us examine text data from party conventions held during the 2012 US Presidential elections. Please keep in mind that this cell may take a few minutes to run, but the results of all that number-crunching are well worth the wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "\n",
    "if \"merge_entities\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe('merge_entities')\n",
    "\n",
    "if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "convention_df = st.SampleCorpora.ConventionData2012.get_data()\n",
    "corpus = st.CorpusFromPandas(convention_df, category_col=\"party\", text_col=\"text\", nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>democrat</td>\n",
       "      <td>Thank you. Thank you. Thank you. Thank you so ...</td>\n",
       "      <td>BARACK OBAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>democrat</td>\n",
       "      <td>Thank you so much. Tonight, I am so thrilled a...</td>\n",
       "      <td>MICHELLE OBAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>Thank you. It is a singular honor to be here t...</td>\n",
       "      <td>RICHARD DURBIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>Hey, Delaware. \\nAnd my favorite Democrat, Jil...</td>\n",
       "      <td>JOSEPH BIDEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>Hello. \\nThank you, Angie. I'm so proud of how...</td>\n",
       "      <td>JILL BIDEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>republican</td>\n",
       "      <td>As the elected leader of 250,000 College Repub...</td>\n",
       "      <td>ALEX SCHRIVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>republican</td>\n",
       "      <td>Good afternoon. I'm Pete Sessions, a congressm...</td>\n",
       "      <td>PETE SESSIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>republican</td>\n",
       "      <td>To Chairman Priebus and to my fellow Americans...</td>\n",
       "      <td>BOB BUCKHORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>republican</td>\n",
       "      <td>\\nAbsolutely. Thank you, Mr.Chairman.\\nWelcome...</td>\n",
       "      <td>SHARON DAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>republican</td>\n",
       "      <td>I am thrilled to add Utah's voice in support f...</td>\n",
       "      <td>MIA LOVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          party                                               text  \\\n",
       "0      democrat  Thank you. Thank you. Thank you. Thank you so ...   \n",
       "1      democrat  Thank you so much. Tonight, I am so thrilled a...   \n",
       "2      democrat  Thank you. It is a singular honor to be here t...   \n",
       "3      democrat  Hey, Delaware. \\nAnd my favorite Democrat, Jil...   \n",
       "4      democrat  Hello. \\nThank you, Angie. I'm so proud of how...   \n",
       "..          ...                                                ...   \n",
       "184  republican  As the elected leader of 250,000 College Repub...   \n",
       "185  republican  Good afternoon. I'm Pete Sessions, a congressm...   \n",
       "186  republican  To Chairman Priebus and to my fellow Americans...   \n",
       "187  republican  \\nAbsolutely. Thank you, Mr.Chairman.\\nWelcome...   \n",
       "188  republican  I am thrilled to add Utah's voice in support f...   \n",
       "\n",
       "            speaker  \n",
       "0      BARACK OBAMA  \n",
       "1    MICHELLE OBAMA  \n",
       "2    RICHARD DURBIN  \n",
       "3      JOSEPH BIDEN  \n",
       "4        JILL BIDEN  \n",
       "..              ...  \n",
       "184   ALEX SCHRIVER  \n",
       "185   PETE SESSIONS  \n",
       "186    BOB BUCKHORN  \n",
       "187      SHARON DAY  \n",
       "188        MIA LOVE  \n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the **corpus** ready, generate an interactive visualization in HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus, category=\"democrat\", \n",
    "                                       category_name=\"Democratic\",\n",
    "                                       not_category_name=\"Republican\",\n",
    "                                       width_in_pixels=1000,\n",
    "                                       metadata=convention_df[\"speaker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll render the HTML—give it a minute or two to load, it's worth the wait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"foo.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1205cf730>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"foo.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider storing text from the last three years of customer support for a specific product in your company. Assume your team required information about how customers were discussing the product. This scattertext library has the potential to be extremely useful! You could cluster (k=2) on NPS (a customer evaluation metric) and then substitute the top two clustering components for the Democrat/Republican dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this blog, we introduce some frequently used features of spaCy from an overall perspective. If you asked many data scientists five years ago about open source in Python for natural language processing, the default answer would have been NLTK. That project includes almost everything except the kitchen sink and has mostly academic components.\n",
    "\n",
    "There's so much more we can [do with spaCy](https://blog.dominodatalab.com/making-pyspark-work-spacy-overcoming-serialization-errors)— hopefully, this tutorial provides an introduction. We wish you all the best in your natural language work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
