{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Amazon Reviews using Scrapy in Python Part 2\n",
    "> Are you looking for a method of scraping Amazon reviews and do not know where to begin with? In that case, you may find this blog very useful in scraping Amazon reviews.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Zeyu Guan\n",
    "- categories: [spaCy, Python, Machine Learning, Data Mining, NLP, RandomForest]\n",
    "- annotations: true\n",
    "- image: https://www.freecodecamp.org/news/content/images/2020/09/wall-5.jpeg\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "[wordcloud](https://github.com/amueller/word_cloud), \n",
    "[geopandas](https://geopandas.org/en/stable/getting_started/install.html), \n",
    "[nbformat](https://pypi.org/project/nbformat/), \n",
    "[seaborn](https://seaborn.pydata.org/installing.html), \n",
    "[scikit-learn](https://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's get started!\n",
    "First thing first, you need to load all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Following the previous blog, the raw data we scraped from Amazon look like below. \n",
    "![Raw Data](https://live.staticflickr.com/65535/51958371521_d139a6c0b1_h.jpg)\n",
    "\n",
    "Even thought that looks relatively clean, but there are still some inperfections such as star1 and star2 need to be combined, date need to be splited, and etc. The whole process could be found from my [github notebooks](https://github.com/christopherGuan/sample-ds-blog).\n",
    "\n",
    "Below is data after cleaning. It contains 6 columns and more than 500 rows. \n",
    "\n",
    "![Clean Data](https://live.staticflickr.com/65535/51930037691_4a23b4c441_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# EDA\n",
    "Below are the questions I curioused about, and the result generated by doing data analysis.\n",
    "\n",
    "- Which rating (1-5) got the most and least?\n",
    "![Which point was rated the most](https://live.staticflickr.com/65535/51929072567_d34db66693_h.jpg)\n",
    "\n",
    "- Which country are they targeting?\n",
    "![Target Country](https://live.staticflickr.com/65535/51930675230_6314e2ccde_h.jpg)\n",
    "\n",
    "- Which month people prefer to give a higher rating?\n",
    "![higher rating](https://live.staticflickr.com/65535/51929085842_0cb0aa6b06_w.jpg)\n",
    "\n",
    "- Which month people leave commons the most?\n",
    "![More commons](https://live.staticflickr.com/65535/51929085857_49f7c889d2_w.jpg )\n",
    "\n",
    "\n",
    "- What are the useful words that people mentioned in the reviews?\n",
    "\n",
    "![More commons](https://live.staticflickr.com/65535/51930471329_82bf0c43b9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Method 1)\n",
    "\n",
    "## What is sentiment analysis?\n",
    "\n",
    "Essentially, sentiment analysis or sentiment classification fall under the broad category of text classification tasks in which you are given a phrase or a list of phrases and your classifier is expected to determine whether the sentiment behind that phrase is positive, negative, or neutral. To keep the problem as a binary classification problem, the third attribute is sometimes ignored. Recent tasks have taken into account sentiments such as \"somewhat positive\" and \"somewhat negative.\" \n",
    "\n",
    "In this specific case, we catogrize 4 and 5 stars to the positive group and 1 & 2 stars to the negative gorup. \n",
    "\n",
    "![rating](https://live.staticflickr.com/65535/51930237493_b6afc18052_c.jpg)\n",
    "\n",
    "Below are the most frequently words in reviews from positive group and negative group respectively. \n",
    "\n",
    "Positive review\n",
    "![positive](https://live.staticflickr.com/65535/51930164126_33b911e6b3_c.jpg)\n",
    "\n",
    "Negative review\n",
    "![negative](https://live.staticflickr.com/65535/51930165221_cf61fce68e_c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the first model\n",
    "\n",
    "Now we can build up a easy model that, as input, it will accept reviews. It will then predict whether the review will be positive or negative.\n",
    "\n",
    "Because this is a classification task, we will train a simple logistic regression model.\n",
    "\n",
    "\n",
    "- **Clean Data**\n",
    "First, we create a new function to remove all punctuations from the data for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"'))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Split the Dataframe**\n",
    "\n",
    "Now, we split 80% of the dataset for training and 20% for testing. Meanwhile, each dataset should contain only two variables, one is to indicate positive or negative and another one is the reviews.\n",
    "\n",
    "![output](https://live.staticflickr.com/65535/51930294148_3a9db0297c_b.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['random_number'] = np.random.randn(len(index))\n",
    "train = df[df['random_number'] <= 0.8]\n",
    "test = df[df['random_number'] > 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Create a bag of words**\n",
    "\n",
    "Here I would like to introduce a new package.\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/install.html) is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.\n",
    "\n",
    "In this example, we are going to use [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) to convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "The reason why we need to convert the text into a bag-of-words model is because the logistic regression algorithm cannot understand text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = vectorizer.fit_transform(train['title'])\n",
    "test_matrix = vectorizer.transform(test['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Import Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Split target and independent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fit model on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Make predictionsa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be either 1 or -1. As we assumed before, 1 presents the model predict the review is a positive review and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now, we can test the accuracy of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![accuracy](https://live.staticflickr.com/65535/51929260132_045027628c_z.jpg)\n",
    "\n",
    "The accuracy is as high as 89%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Method 2)\n",
    "\n",
    "In this process, you will learn how to build your own sentiment analysis classifier using Python and understand the basics of NLP (natural language processing). First, let's try to use a quick and dirty method to utilize the [Naive Bayes classifier](https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python) to predict the sentiments of Amazon product review. \n",
    "\n",
    "Based on the application's requirements, we should first put each review in a txt file and catogorize them as negative or positive review in different folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all negative review\n",
    "neg = df[df[\"sentiment\"] == -1].review\n",
    "\n",
    "#Reset the index\n",
    "neg.index = range(len(neg.index))\n",
    "\n",
    "## Write each DataFrame to separate txt\n",
    "for i in range(len(neg)):          \n",
    "    data = neg[i]\n",
    "    with open(str(i) + \".txt\",\"w\") as file:\n",
    "        file.write(data + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the order of the official data and remove all the content. In other words, we only keep the file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#Get file names\n",
    "file_names = os.listdir('/Users/zeyu/nltk_data/corpora/movie_reviews/neg')\n",
    "\n",
    "#Convert pandas\n",
    "neg_df = pd.DataFrame (file_names, columns = ['file_name'])\n",
    "\n",
    "#split to sort\n",
    "neg_df[['number','id']] = neg_df.file_name.apply(\n",
    "   lambda x: pd.Series(str(x).split(\"_\")))\n",
    "\n",
    "#change the number to be the index\n",
    "neg_df_index = neg_df.set_index('number')\n",
    "neg_org = neg_df_index.sort_index(ascending=True)\n",
    "\n",
    "#del neg[\"id\"]\n",
    "neg_org.reset_index(inplace=True)\n",
    "\n",
    "neg_org = neg_org.drop([0], axis=0).reset_index(drop=True)\n",
    "neg_names = neg_org['file_name']\n",
    "\n",
    "for file_name in neg_names:\n",
    "    t = open(f'/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{file_name}', 'w')\n",
    "    t.write(\"\")\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we insert the content of amazon review to the official files with their original file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get file names\n",
    "file_names = os.listdir('/Users/zeyu/Desktop/DS/neg')\n",
    "\n",
    "#Convert pandas\n",
    "pos_df = pd.DataFrame (file_names, columns = ['file_name'])\n",
    "\n",
    "pos_names = pos_df['file_name']\n",
    "\n",
    "for index, file_name in enumerate(pos_names):\n",
    "    try: \n",
    "\n",
    "        t = open(f'/Users/zeyu/Desktop/DS/neg/{file_name}', 'r')\n",
    "        # t.write(\"\")\n",
    "        t_val = ascii(t.read())\n",
    "        t.close()\n",
    "        \n",
    "        writefname = pos_names_org[index]\n",
    "        t = open(f'/Users/zeyu/nltk_data/corpora/movie_reviews/neg/{writefname}', 'w')\n",
    "        t.write(t_val)\n",
    "        t.close()\n",
    "    except:\n",
    "        print(f'{index} Reading/writing Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we can just run these few lines to predict the sentiments of Amazon product review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#All words, not unique.          \n",
    "random.shuffle(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to lower case. Count word appears.\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "\n",
    "#Only show first 2000.\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the accuracy of the given.\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
